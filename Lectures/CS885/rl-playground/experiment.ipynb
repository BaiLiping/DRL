{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from envs import ObservableEnv, env_gridworld\n",
    "import rl.mdp as mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slides Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Environment as torch tensor\n",
    "\n",
    "* Reward:\n",
    "    A 2D array of the rewards of $[s, a]$\n",
    "\n",
    "* Transition:\n",
    "    A 3D array of the probability of reaching state $s'$ under state $s$ and taking aciton $a$\n",
    "    \n",
    "* Value:\n",
    "    A 1D array of the expected value at state $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['PU', 'PF', 'RU', 'RF']\n",
    "actions = ['S', 'A']\n",
    "rewards = torch.tensor([[0, 0],\n",
    "                        [0, 0],\n",
    "                        [10, 10],\n",
    "                        [10, 10]], dtype=torch.float32)\n",
    "transitions = torch.tensor([[[1, 0, 0, 0], [0.5, 0.5, 0, 0]],\n",
    "                            [[0.5, 0., 0., 0.5], [0., 1., 0., 0.]],\n",
    "                            [[0.5, 0., 0.5, 0.], [0.5, 0.5, 0., 0.]],\n",
    "                            [[0., 0., 0.5, 0.5], [0., 1., 0., 0.]]], dtype=torch.float32)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ObservableEnv(states, actions, transitions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 90 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([31.5824, 38.6013, 44.0214, 54.1988]), tensor([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([0, 0, 10, 10], dtype=torch.float32)\n",
    "\n",
    "value_iteration = mdp.ValueIteration(env, value, gamma)\n",
    "value_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 2 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.0250,  4.5000, 16.5250, 21.0250]), tensor([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([0, 0, 10, 10], dtype=torch.float32)\n",
    "policy = torch.tensor([random.randint(0, 1) for _ in range(4)])\n",
    "\n",
    "policy_iteration = mdp.PolicyIteration(env, value, policy, gamma)\n",
    "policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 19 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([31.5849, 38.6038, 44.0239, 54.2014]), tensor([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([0, 0, 10, 10], dtype=torch.float32)\n",
    "policy = torch.tensor([random.randint(0, 1) for _ in range(4)])\n",
    "n_evals = 5\n",
    "\n",
    "modified_policy_iteration = mdp.ModifiedPolicyIteration(env, value, policy, gamma, n_evals)\n",
    "modified_policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 1 Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Environment\n",
    "\n",
    "Grid world layout:\n",
    "\n",
    "  \\---------------------  \n",
    "  |  0 |  1 |  2 |  3 |  \n",
    "  \\---------------------  \n",
    "  |  4 |  5 |  6 |  7 |  \n",
    "  \\---------------------  \n",
    "  |  8 |  9 | 10 | 11 |  \n",
    "  \\---------------------  \n",
    "  | 12 | 13 | 14 | 15 |  \n",
    "  \\---------------------  \n",
    "\n",
    "  Goal state: 15   \n",
    "  Bad state: 9  \n",
    "  End state: 16  \n",
    " \n",
    "$|S| = 17$  \n",
    "$|A| = 4$\n",
    "\n",
    "transitions: $|S| * |A| * |S'|$  \n",
    "rewards = $|S| * |A|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [str(i) for i in range(17)]\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "transitions = env_gridworld.transtions\n",
    "rewards = env_gridworld.rewards\n",
    "\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ObservableEnv(states, actions, transitions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 23 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 39.6668,  46.4196,  54.0687,  61.7166,  40.8314,  47.9155,  62.9674,\n",
       "          72.6333,  41.8038,  -6.6523,  73.7750,  85.3184,  55.0558,  65.7478,\n",
       "          85.3184, 100.0000,   0.0000]),\n",
       " tensor([3, 3, 1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.empty(len(states), dtype=torch.float32)\n",
    "\n",
    "value_iteration = mdp.ValueIteration(env, value, gamma)\n",
    "value_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 9 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([4.9419e+01, 5.9734e+01, 6.8967e+01, 7.5514e+01, 5.1799e+01, 6.2376e+01,\n",
       "         7.6428e+01, 8.3726e+01, 5.5216e+01, 6.5897e+00, 8.4529e+01, 9.1685e+01,\n",
       "         6.7922e+01, 7.6410e+01, 9.1685e+01, 1.0000e+02, 1.1351e-43]),\n",
       " tensor([3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.empty(len(states), dtype=torch.float32)\n",
    "policy = torch.tensor([random.randint(0, len(actions) - 1) for _ in range(len(states))], dtype=torch.int64)\n",
    "\n",
    "policy_iteration = mdp.PolicyIteration(env, value, policy, gamma)\n",
    "policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 7 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([6.0633e+01, 6.6039e+01, 7.1806e+01, 7.7093e+01, 5.9819e+01, 6.5185e+01,\n",
       "         7.7832e+01, 8.4141e+01, 5.8096e+01, 7.9886e+00, 8.4867e+01, 9.1782e+01,\n",
       "         6.9497e+01, 7.6810e+01, 9.1782e+01, 1.0000e+02, 1.2612e-44]),\n",
       " tensor([3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.empty(len(states))\n",
    "policy = torch.tensor([random.randint(0, len(actions) - 1) for _ in range(len(states))])\n",
    "n_evals = 5\n",
    "\n",
    "modified_policy_iteration = mdp.ModifiedPolicyIteration(env, value, policy, gamma, n_evals)\n",
    "modified_policy_iteration.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
