{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:29:07.683798Z",
     "start_time": "2019-04-22T03:29:06.967885Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Masked softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:29:07.690646Z",
     "start_time": "2019-04-22T03:29:07.685803Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "# X: 3-D tensor, valid_length: 1-D or 2-D tensor\n",
    "def masked_softmax(X, valid_length):\n",
    "    if valid_length is None:\n",
    "        return X.softmax()\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_length.ndim == 1:\n",
    "            valid_length = valid_length.repeat(shape[1], axis=0)\n",
    "        else:\n",
    "            valid_length = valid_length.reshape((-1,))\n",
    "        # fill masked elements with a large negative, whose exp is 0\n",
    "        X = nd.SequenceMask(X.reshape((-1, shape[-1])), valid_length, True, \n",
    "                            axis=1, value=-1e6)\n",
    "        return X.softmax().reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:29:07.714655Z",
     "start_time": "2019-04-22T03:29:07.692276Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[0.488994   0.511006   0.         0.        ]\n",
       "  [0.43654838 0.56345165 0.         0.        ]]\n",
       "\n",
       " [[0.28817102 0.3519408  0.3598882  0.        ]\n",
       "  [0.29034293 0.25239873 0.45725834 0.        ]]]\n",
       "<NDArray 2x2x4 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(nd.random.uniform(shape=(2,2,4)), nd.array([2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dot Product Attention\n",
    "\n",
    "$$\\alpha(\\mathbf Q, \\mathbf K) = \\langle \\mathbf Q, \\mathbf K^T \\rangle /\\sqrt{d}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:29:07.727322Z",
     "start_time": "2019-04-22T03:29:07.722556Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Block):  \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # valid_length: either (batch_size, ) or (batch_size, seq_len) \n",
    "    def forward(self, query, key, value, valid_length=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        scores = nd.batch_dot(query, key, transpose_b=True) / math.sqrt(d)\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        return nd.batch_dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:29:07.736918Z",
     "start_time": "2019-04-22T03:29:07.728641Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[ 2.        3.        4.        5.      ]]\n",
       "\n",
       " [[10.       11.       12.000001 13.      ]]]\n",
       "<NDArray 2x1x4 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = DotProductAttention(dropout=0.5)\n",
    "atten.initialize()\n",
    "keys = nd.ones((2,10,2))\n",
    "values = nd.arange(40).reshape((1,10,4)).repeat(2,axis=0)\n",
    "atten(nd.ones((2,1,2)), keys, values, nd.array([2, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multilayer Perception Attention\n",
    "\n",
    "$\\mathbf W_k\\in\\mathbb R^{h\\times d_k}$, $\\mathbf W_q\\in\\mathbb R^{h\\times d_q}$, and $\\mathbf v\\in\\mathbb R^{p}$:\n",
    "\n",
    "$$\\alpha(\\mathbf k, \\mathbf q) = \\mathbf v^T \\text{tanh}(\\mathbf W_k \\mathbf k + \\mathbf W_q\\mathbf q). $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:29:07.744783Z",
     "start_time": "2019-04-22T03:29:07.738228Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Block):  # This class is saved in d2l. \n",
    "    def __init__(self, units, dropout, **kwargs):\n",
    "        super(MLPAttention, self).__init__(**kwargs)\n",
    "        # Use flatten=True to keep query's and key's 3-D shapes.   \n",
    "        self.W_k = nn.Dense(units, activation='tanh', \n",
    "                            use_bias=False, flatten=False)\n",
    "        self.W_q = nn.Dense(units, activation='tanh', \n",
    "                            use_bias=False, flatten=False)\n",
    "        self.v = nn.Dense(1, use_bias=False, flatten=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        query, key = self.W_k(query), self.W_q(key)\n",
    "        # expand query to (batch_size, #querys, 1, units), and key to \n",
    "        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.  \n",
    "        features = query.expand_dims(axis=2) + key.expand_dims(axis=1)\n",
    "        scores = self.v(features).squeeze(axis=-1)\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        return nd.batch_dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:29:07.757874Z",
     "start_time": "2019-04-22T03:29:07.746575Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[ 2.        3.        4.        5.      ]]\n",
       "\n",
       " [[10.       11.       12.000001 13.      ]]]\n",
       "<NDArray 2x1x4 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = MLPAttention(units=8, dropout=0.1)\n",
    "atten.initialize()\n",
    "atten(nd.ones((2,1,2)), keys, values, nd.array([2, 6]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
