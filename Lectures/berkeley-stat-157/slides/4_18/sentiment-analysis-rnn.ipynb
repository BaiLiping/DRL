{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Sentiment Classification: Using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:36:50.290407Z",
     "start_time": "2019-04-18T18:36:49.391263Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import d2l\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn, utils as gutils\n",
    "from mxnet.contrib import text\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Sentiment Classification Data\n",
    "\n",
    "###  Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:06.846180Z",
     "start_time": "2019-04-18T18:36:50.292250Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "23"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "fname = gutils.download(url, data_dir)\n",
    "with tarfile.open(fname, 'r') as f:\n",
    "    f.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Read the training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:07.727569Z",
     "start_time": "2019-04-18T18:37:06.848226Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "24"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainings: 25000 \n",
      "# tests: 25000\n",
      "label: 1 review: Normally the best way to annoy me in a film is to include so\n",
      "label: 1 review: The Bible teaches us that the love of money is the root of a\n",
      "label: 1 review: Being someone who lists Night of the Living Dead at number t\n"
     ]
    }
   ],
   "source": [
    "def read_imdb(folder='train'):\n",
    "    data, labels = [], []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_dir, 'aclImdb', folder, label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data, test_data = read_imdb('train'), read_imdb('test')\n",
    "print('# trainings:', len(train_data[0]), '\\n# tests:', len(test_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('label:', y, 'review:', x[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization and Vocabulary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:10.750284Z",
     "start_time": "2019-04-18T18:37:07.729641Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    return [line.split(' ') for line in sentences]\n",
    "\n",
    "train_tokens = tokenize(train_data[0])\n",
    "test_tokens = tokenize(test_data[0])\n",
    "\n",
    "vocab = d2l.Vocab([tk for line in train_tokens for tk in line], min_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Padding to the Same Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.118289Z",
     "start_time": "2019-04-18T18:37:10.751950Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "44"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "\n",
    "def pad(x):\n",
    "    if len(x) > max_len:        \n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [vocab.unk] * (max_len - len(x))\n",
    "    \n",
    "train_features = nd.array([pad(vocab[line]) for line in train_tokens])\n",
    "test_features = nd.array([pad(vocab[line]) for line in test_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.123639Z",
     "start_time": "2019-04-18T18:37:20.119937Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(train_features, train_data[1])\n",
    "test_set = gdata.ArrayDataset(test_features, test_data[1])\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Print the shape of the first mini-batch of data and the number of mini-batches in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.138763Z",
     "start_time": "2019-04-18T18:37:20.125156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (64, 500) y (64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('# batches:', 391)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'# batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Use a Recurrent Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.147984Z",
     "start_time": "2019-04-18T18:37:20.140947Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "46"
    }
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Set Bidirectional to True to get a bidirectional recurrent neural\n",
    "        # network\n",
    "        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True, input_size=embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The shape of inputs is (batch size, number of words). Because LSTM\n",
    "        # needs to use sequence as the first dimension, the input is\n",
    "        # transformed and the word feature is then extracted. The output shape\n",
    "        # is (number of words, batch size, word vector dimension).\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        # The shape of states is (number of words, batch size, 2 * number of\n",
    "        # hidden units).\n",
    "        states = self.encoder(embeddings)\n",
    "        # Concatenate the hidden states of the initial time step and final\n",
    "        # time step to use as the input of the fully connected layer. Its\n",
    "        # shape is (batch size, 4 * number of hidden units)\n",
    "        encoding = nd.concat(states[0], states[-1])\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create a bidirectional recurrent neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:25.447434Z",
     "start_time": "2019-04-18T18:37:20.149858Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, ctx = 100, 100, 2, d2l.try_all_gpus()\n",
    "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)\n",
    "net.initialize(init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load Pre-trained Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:40.206184Z",
     "start_time": "2019-04-18T18:37:25.449068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49339, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.6B.100d.txt')\n",
    "embeds = glove_embedding.get_vecs_by_tokens(vocab.idx_to_token)\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use these word vectors as feature vectors for each word in the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:40.210859Z",
     "start_time": "2019-04-18T18:37:40.207714Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "47"
    }
   },
   "outputs": [],
   "source": [
    "net.embedding.weight.set_data(embeds)\n",
    "net.embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train and Evaluate the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.801349Z",
     "start_time": "2019-04-18T18:37:40.212205Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "48"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [gpu(0), gpu(1)]\n",
      "epoch 1, loss 0.5948, train acc 0.660, test acc 0.811, time 41.8 sec\n",
      "epoch 2, loss 0.4026, train acc 0.822, test acc 0.836, time 41.6 sec\n",
      "epoch 3, loss 0.3604, train acc 0.843, test acc 0.844, time 42.8 sec\n",
      "epoch 4, loss 0.3320, train acc 0.859, test acc 0.842, time 42.4 sec\n",
      "epoch 5, loss 0.3044, train acc 0.870, test acc 0.853, time 41.0 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.806821Z",
     "start_time": "2019-04-18T18:41:09.802933Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "49"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    sentence = nd.array(vocab[sentence.split()], ctx=d2l.try_gpu())\n",
    "    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, use the trained model to classify the sentiments of two simple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.814658Z",
     "start_time": "2019-04-18T18:41:09.808150Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "50"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.821180Z",
     "start_time": "2019-04-18T18:41:09.816015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
