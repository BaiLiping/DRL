{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "72"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # Use autograd to determine whether the current mode is training mode or prediction mode.\n",
    "    if not autograd.is_training():\n",
    "        # use the moving average\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:  # fully connected layer\n",
    "            mean = X.mean(axis=0)\n",
    "            var = ((X - mean) ** 2).mean(axis=0)\n",
    "        else:                  # convolution, hence per layer\n",
    "            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "        # In training mode, the current mean and variance are used for the standardization.\n",
    "        X_hat = (X - mean) / nd.sqrt(var + eps)\n",
    "        # Update the mean and variance of the moving average.\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift.\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BatchNorm Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "73"
    }
   },
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self, num_features, num_dims, **kwargs):\n",
    "        super(BatchNorm, self).__init__(**kwargs)\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter involved in gradient finding and iteration are initialized to 0 and 1 respectively.\n",
    "        self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n",
    "        self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n",
    "        # All the variables not involved in gradient finding and iteration are initialized to 0 on the CPU.\n",
    "        self.moving_mean = nd.zeros(shape)\n",
    "        self.moving_var = nd.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the CPU, copy moving_mean and moving_var to the device where X is located.\n",
    "        if self.moving_mean.context != X.context:\n",
    "            self.moving_mean = self.moving_mean.copyto(X.context)\n",
    "            self.moving_var = self.moving_var.copyto(X.context)\n",
    "        # Save the updated moving_mean and moving_var.\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma.data(), self.beta.data(), self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LeNet with Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "74"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size=5),\n",
    "        BatchNorm(6, num_dims=4),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(16, kernel_size=5),\n",
    "        BatchNorm(16, num_dims=4),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Dense(120),\n",
    "        BatchNorm(120, num_dims=2),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.Dense(84),\n",
    "        BatchNorm(84, num_dims=2),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Batch Norm LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "77"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.6847, train acc 0.752, test acc 0.792, time 3.7 sec\n",
      "epoch 2, loss 0.4129, train acc 0.850, test acc 0.855, time 3.6 sec\n",
      "epoch 3, loss 0.3586, train acc 0.870, test acc 0.866, time 3.6 sec\n",
      "epoch 4, loss 0.3320, train acc 0.879, test acc 0.874, time 3.5 sec\n",
      "epoch 5, loss 0.3108, train acc 0.888, test acc 0.864, time 3.5 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size, ctx = 1.0, 5, 256, d2l.try_gpu()\n",
    "net.initialize(ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's have a look at the scale parameter `gamma` and the shift parameter `beta` learned from the first batch normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "60"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1.7873654  0.61735505 1.7933072  1.687522   1.5147648  1.6866305 ]\n",
       " <NDArray 6 @gpu(0)>, \n",
       " [ 0.91468376  0.50423574 -0.01995357  0.7233319  -0.65631443 -1.8838943 ]\n",
       " <NDArray 6 @gpu(0)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.data().reshape((-1,)), net[1].beta.data().reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Norm in Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size=5),\n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(16, kernel_size=5),\n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Dense(120),\n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.Dense(84),\n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the same hyper-parameter to carry out the training. Note that as always the Gluon variant runs a lot faster since the code that is being executed is compiled C++/CUDA rather than interpreted Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.6533, train acc 0.768, test acc 0.802, time 2.3 sec\n",
      "epoch 2, loss 0.4023, train acc 0.854, test acc 0.876, time 2.4 sec\n",
      "epoch 3, loss 0.3536, train acc 0.872, test acc 0.863, time 2.4 sec\n",
      "epoch 4, loss 0.3251, train acc 0.883, test acc 0.885, time 2.4 sec\n",
      "epoch 5, loss 0.3070, train acc 0.889, test acc 0.871, time 2.3 sec\n"
     ]
    }
   ],
   "source": [
    "net.initialize(ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
