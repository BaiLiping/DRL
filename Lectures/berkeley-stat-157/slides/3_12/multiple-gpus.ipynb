{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-GPU Computation with Data Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 19 00:41:21 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla M60           Off  | 00000000:00:1D.0 Off |                    0 |\r\n",
      "| N/A   31C    P0    38W / 150W |   2639MiB /  7618MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla M60           Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   33C    P8    13W / 150W |     11MiB /  7618MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "import time\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define the Model: LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "scale = 0.01\n",
    "W1 = nd.random.normal(scale=scale, shape=(20, 1, 3, 3))\n",
    "b1 = nd.zeros(shape=20)\n",
    "W2 = nd.random.normal(scale=scale, shape=(50, 20, 5, 5))\n",
    "b2 = nd.zeros(shape=50)\n",
    "W3 = nd.random.normal(scale=scale, shape=(800, 128))\n",
    "b3 = nd.zeros(shape=128)\n",
    "W4 = nd.random.normal(scale=scale, shape=(128, 10))\n",
    "b4 = nd.zeros(shape=10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "def lenet(X, params):\n",
    "    h1_conv = nd.Convolution(data=X, weight=params[0], bias=params[1],\n",
    "                             kernel=(3, 3), num_filter=20)\n",
    "    h1_activation = nd.relu(h1_conv)\n",
    "    h1 = nd.Pooling(data=h1_activation, pool_type='avg', kernel=(2, 2),\n",
    "                    stride=(2, 2))\n",
    "    h2_conv = nd.Convolution(data=h1, weight=params[2], bias=params[3],\n",
    "                             kernel=(5, 5), num_filter=50)\n",
    "    h2_activation = nd.relu(h2_conv)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type='avg', kernel=(2, 2),\n",
    "                    stride=(2, 2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    h3_linear = nd.dot(h2, params[4]) + params[5]\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    y_hat = nd.dot(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Copy Parameter to a Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def get_params(params, ctx):\n",
    "    new_params = [p.copyto(ctx) for p in params]\n",
    "    for p in new_params:\n",
    "        p.attach_grad()\n",
    "    return new_params\n",
    "\n",
    "new_params = get_params(params, mx.gpu(0))\n",
    "print('b1 weight:', new_params[1])\n",
    "print('b1 grad:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sum Over All Devices and then Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].copyto(data[0].context)\n",
    "    for i in range(1, len(data)):\n",
    "        data[0].copyto(data[i])\n",
    "        \n",
    "data = [nd.ones((1, 2), ctx=mx.gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('before allreduce:', data)\n",
    "allreduce(data)\n",
    "print('after allreduce:', data)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split a Data Batch into Each GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def split_and_load(data, ctx):\n",
    "    n, k = data.shape[0], len(ctx)\n",
    "    m = n // k  # For simplicity, we assume the data is divisible.\n",
    "    assert m * k == n, '# examples is not divided by # devices.'\n",
    "    return [data[i * m: (i + 1) * m].as_in_context(ctx[i]) for i in range(k)]\n",
    "\n",
    "batch = nd.arange(24).reshape((6, 4))\n",
    "ctx = [mx.gpu(0), mx.gpu(1)]\n",
    "splitted = split_and_load(batch, ctx)\n",
    "print('input: ', batch)\n",
    "print('load into', ctx)\n",
    "print('output:', splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU Training on a Single Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(X, y, gpu_params, ctx, lr):\n",
    "    # When ctx contains multiple GPUs, mini-batches of data instances are divided and copied to each GPU.\n",
    "    gpu_Xs, gpu_ys = split_and_load(X, ctx), split_and_load(y, ctx)\n",
    "    with autograd.record():  # Loss is calculated separately on each GPU.\n",
    "        ls = [loss(lenet(gpu_X, gpu_W), gpu_y)\n",
    "              for gpu_X, gpu_y, gpu_W in zip(gpu_Xs, gpu_ys, gpu_params)]\n",
    "    for l in ls:  # Back Propagation is performed separately on each GPU.\n",
    "        l.backward()\n",
    "    # Add up all the gradients from each GPU and then broadcast them to all the GPUs.\n",
    "    for i in range(len(gpu_params[0])):\n",
    "        allreduce([gpu_params[c][i].grad for c in range(len(ctx))])\n",
    "    for param in gpu_params:  # The model parameters are updated separately on each GPU.\n",
    "        d2l.sgd(param, lr, X.shape[0])  # Here, we use a full-size batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "    print('running on:', ctx)\n",
    "    # Copy model parameters to num_gpus GPUs.\n",
    "    gpu_params = [get_params(params, c) for c in ctx]\n",
    "    for epoch in range(4):\n",
    "        start = time.time()\n",
    "        for X, y in train_iter:\n",
    "            # Perform multi-GPU training for a single mini-batch.\n",
    "            train_batch(X, y, gpu_params, ctx, lr)\n",
    "            nd.waitall()\n",
    "        train_time = time.time() - start\n",
    "\n",
    "        def net(x):  # Verify the model on GPU 0.\n",
    "            return lenet(x, gpu_params[0])\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy(test_iter, net, ctx[0])\n",
    "        print('epoch %d, time: %.1f sec, test acc: %.2f'\n",
    "              % (epoch + 1, train_time, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU Training Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on: [gpu(0)]\n",
      "epoch 1, time: 2.7 sec, test acc: 0.10\n",
      "epoch 2, time: 2.2 sec, test acc: 0.66\n",
      "epoch 3, time: 2.2 sec, test acc: 0.75\n",
      "epoch 4, time: 2.2 sec, test acc: 0.71\n"
     ]
    }
   ],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on: [gpu(0), gpu(1)]\n",
      "epoch 1, time: 2.5 sec, test acc: 0.10\n",
      "epoch 2, time: 2.2 sec, test acc: 0.61\n",
      "epoch 3, time: 2.2 sec, test acc: 0.75\n",
      "epoch 4, time: 2.2 sec, test acc: 0.76\n"
     ]
    }
   ],
   "source": [
    "train(num_gpus=2, batch_size=256, lr=0.2)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
