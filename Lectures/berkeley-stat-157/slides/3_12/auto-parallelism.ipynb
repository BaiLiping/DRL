{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computation using CPUs and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x):\n",
    "    return [nd.dot(x, x) for _ in range(10)]\n",
    "\n",
    "x_cpu = nd.random.uniform(shape=(2000, 2000))\n",
    "x_gpu = nd.random.uniform(shape=(6000, 6000), ctx=mx.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the two NDArrays to run the `run` function on both the CPU and GPU and print the time required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on CPU. time: 0.3592 sec\n",
      "Then run on GPU. time: 1.2187 sec\n"
     ]
    }
   ],
   "source": [
    "run(x_cpu)  # Warm-up begins.\n",
    "run(x_gpu)\n",
    "nd.waitall()  # Warm-up ends.\n",
    "\n",
    "with d2l.Benchmark('Run on CPU.'):\n",
    "    run(x_cpu)\n",
    "    nd.waitall()\n",
    "\n",
    "with d2l.Benchmark('Then run on GPU.'):\n",
    "    run(x_gpu)\n",
    "    nd.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove `nd.waitall()` between the two computing tasks `run(x_cpu)` and `run(x_gpu)` and hope the system can automatically parallel these two tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on both CPU and GPU in parallel. time: 1.2227 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('Run on both CPU and GPU in parallel.'):\n",
    "    run(x_cpu)\n",
    "    run(x_gpu)\n",
    "    nd.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, when two computing tasks are executed together, the total execution time is less than the sum of their separate execution times. This means that MXNet can effectively automate parallel computation on CPUs and GPUs.\n",
    "\n",
    "\n",
    "## Parallel Computation of Computing and Communication\n",
    "\n",
    "In computations that use both the CPU and GPU, we often need to copy data between the CPU and GPU, resulting in data communication. In the example below, we compute on the GPU and then copy the results back to the CPU. We print the GPU computation time and the communication time from the GPU to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU. time: 1.2269 sec\n",
      "Then copy to CPU. time: 0.5156 sec\n"
     ]
    }
   ],
   "source": [
    "def copy_to_cpu(x):\n",
    "    return [y.copyto(mx.cpu()) for y in x]\n",
    "\n",
    "with d2l.Benchmark('Run on GPU.'):\n",
    "    y = run(x_gpu)\n",
    "    nd.waitall()\n",
    "\n",
    "with d2l.Benchmark('Then copy to CPU.'):\n",
    "    copy_to_cpu(y)\n",
    "    nd.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the `waitall` function between computation and communication and print the total time need to complete both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run and copy in parallel. time: 1.2807 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('Run and copy in parallel.'):\n",
    "    y = run(x_gpu)\n",
    "    copy_to_cpu(y)\n",
    "    nd.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the total time required to perform computation and communication is less than the sum of their separate execution times. It should be noted that this computation and communication task is different from the parallel computation task that simultaneously used the CPU and GPU described earlier in this section. Here, there is a dependency between execution and communication: `y[i]` must be computed before it can be copied to the CPU. Fortunately, the system can copy `y[i-1]` when computing `y[i]` to reduce the total running time of computation and communication.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* MXNet can improve computing performance through automatic parallel computation, such as parallel computation using the CPU and GPU and the parallelization of computation and communication.\n",
    "\n",
    "\n",
    "## Problems\n",
    "\n",
    "* 10 operations were performed in the `run` function defined in this section. There are no dependencies between them. Design an experiment to see if MXNet will automatically execute them in parallel.\n",
    "* Designing computation tasks that include more complex data dependencies, and run experiments to see if MXNet can obtain the correct results and improve computing performance.\n",
    "* When the computational load of an operator is small enough, parallel computation on only the CPU or a single GPU may also improve the computing performance. Design an experiment to verify this.\n",
    "\n",
    "## Discuss on our Forum\n",
    "\n",
    "<div id=\"discuss\" topic_id=\"2382\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
