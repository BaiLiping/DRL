{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CTC.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"taSSqLtNRLaL","colab_type":"text"},"source":["# Training with `nn.CTCLoss`\n","\n","Starting from PyTorch 1.1.0, built-in support for CTC loss is available as `nn.CTCLoss`. Before that, people have to use third-party libraries like `warp-ctc`. We strongly recommend you to use a recent PyTorch version and `nn.CTCLoss` for HW3P2.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HdOqR2rl9W2e","colab_type":"text"},"source":["## Toy task: English spelling to pronunciation\n","\n","As a demonstration, we consider the task of predicting the pronunciation (as sequence of phonemes) of an English word given its spelling. The model we use is a bidirectional LSTM.\n","\n","CTC is actually not the best formulation for this problem, since the letter \"X\" corresponds to two phonemes \"K S\", but it works well with our simplified data."]},{"cell_type":"code","metadata":{"id":"i4n6x1c6RK5t","colab_type":"code","colab":{}},"source":["# Words with only E, I, N, S, T.\n","# Pronunciation is from http://www.speech.cs.cmu.edu/cgi-bin/pronounce\n","data = [\n","    ('SEE', 'S IY'),\n","    ('SET', 'S EH T'),\n","    ('SIT', 'S IH T'),\n","    ('SITE', 'S AY T'),\n","    ('SIN', 'S IH N'),\n","    ('TEEN', 'T IY N'),\n","    ('TIN', 'T IH N'),\n","    ('TIE', 'T AY'),\n","    ('TEST', 'T EH S T'),\n","    ('NET', 'N EH T'),\n","    ('NEET', 'N IY T'),\n","    ('NINE', 'N AY N')\n","]\n","letters = 'EINST'\n","# Starts with ' ' for blank, followed by actual phonemes\n","phonemes = [' ', 'S', 'T', 'N', 'IY', 'IH', 'EH', 'AY']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPJuz7AMC7uT","colab_type":"text"},"source":["Note that, if there are P phonemes for the output, they should be indexed as 1 to P, not 0 to P-1. **Index 0 is reserved for \"blank\".**\n","\n","Accordingly, the output classifier of the model should have P+1 classes, since \"blank\" is also a class."]},{"cell_type":"code","metadata":{"id":"DvetNUFwrQqd","colab_type":"code","outputId":"ecedeb67-5e34-4f91-c524-3a116f83f31c","executionInfo":{"status":"ok","timestamp":1571616475677,"user_tz":240,"elapsed":1147,"user":{"displayName":"Parth Shah","photoUrl":"","userId":"06799060630439446761"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import torch\n","from torch import nn\n","from torch.nn.utils.rnn import *\n","\n","X = [torch.LongTensor([letters.find(c) for c in word]) for word, _ in data]\n","Y = [torch.LongTensor([phonemes.index(p) for p in pron.split()]) for _, pron in data]\n","X_lens = torch.LongTensor([len(seq) for seq in X])\n","Y_lens = torch.LongTensor([len(seq) for seq in Y])\n","X = pad_sequence(X)\n","# `batch_first=True` is required for use in `nn.CTCLoss`.\n","Y = pad_sequence(Y, batch_first=True)\n","\n","print('X', X.size(), X_lens)\n","print('Y', Y.size(), Y_lens)\n","\n","class Model(nn.Module):\n","    def __init__(self, in_vocab, out_vocab, embed_size, hidden_size):\n","        super(Model, self).__init__()\n","        self.embed = nn.Embedding(in_vocab, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n","        self.output = nn.Linear(hidden_size * 2, out_vocab)\n","    \n","    def forward(self, X, lengths):\n","        X = self.embed(X)\n","        packed_X = pack_padded_sequence(X, lengths, enforce_sorted=False)\n","        packed_out = self.lstm(packed_X)[0]\n","        out, out_lens = pad_packed_sequence(packed_out)\n","        # Log softmax after output layer is required for use in `nn.CTCLoss`.\n","        out = self.output(out).log_softmax(2)\n","        return out, out_lens"],"execution_count":2,"outputs":[{"output_type":"stream","text":["X torch.Size([4, 12]) tensor([3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 4, 4])\n","Y torch.Size([12, 4]) tensor([2, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mNBBNx-xEMc7","colab_type":"text"},"source":["## Usage\n","\n","The official documentation is your best friend: https://pytorch.org/docs/stable/nn.html#ctcloss\n","\n","`nn.CTCLoss` takes 4 arguments to compute the loss:\n","* `log_probs`: Prediction of your model at each time step.\n","  * Shape: (T, N, C), where T is the largest length in the batch, N is batch size, and C is number of classes (remember that it should be number of phonemes plus 1).\n","  * **Values must be log probabilities.** Neither probabilities nor logits will work. Make sure the output of your network is log probabilities, by adding a `nn.LogSoftmax` after the last linear layer.\n","* `targets`: The ground truth sequences.\n","  * Shape: (N, S), where N is batch size, and S is the largest length in the batch. **WARNING!** This dimension order is unconventional in PyTorch. If you use `torch.nn.utils.rnn.pad_sequence` to pad the target sequence,  **you must explicitly set `batch_first=True`**.\n","  * Values are indices of phonemes. Again, remember that index 0 is reserved for \"blank\" and should not represent any phoneme.\n","* `input_lengths`: Lengths of sequences in `log_probs`.\n","  * Shape: (N,).\n","  * This is not necessarily the same as lengths of input of the model. If your model uses CNNs or pyramidal RNNs, it changes the length of sequences, and you must correctly compute the lengths of its output to be used here.\n","* `target_lengths`: Lengths of sequences in `targets`.\n","  * Shape: (N,).\n"]},{"cell_type":"code","metadata":{"id":"g5tMaKTx9EiS","colab_type":"code","outputId":"fc704f51-d84b-4ac7-a76a-f1ecbac81594","executionInfo":{"status":"ok","timestamp":1571616478385,"user_tz":240,"elapsed":1541,"user":{"displayName":"Parth Shah","photoUrl":"","userId":"06799060630439446761"}},"colab":{"base_uri":"https://localhost:8080/","height":910}},"source":["torch.manual_seed(11785)\n","model = Model(len(letters), len(phonemes), 4, 4)\n","criterion = nn.CTCLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","\n","for epoch in range(50):\n","    model.zero_grad()\n","    out, out_lens = model(X, X_lens)\n","    loss = criterion(out, Y, out_lens, Y_lens)\n","    print('Epoch', epoch + 1, 'Loss', loss.item())\n","    loss.backward()\n","    optimizer.step()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Epoch 1 Loss 2.1582701206207275\n","Epoch 2 Loss 2.0011541843414307\n","Epoch 3 Loss 1.8661246299743652\n","Epoch 4 Loss 1.7121591567993164\n","Epoch 5 Loss 1.5538407564163208\n","Epoch 6 Loss 1.4120980501174927\n","Epoch 7 Loss 1.2837295532226562\n","Epoch 8 Loss 1.1756290197372437\n","Epoch 9 Loss 1.070193886756897\n","Epoch 10 Loss 0.9620561599731445\n","Epoch 11 Loss 0.8778546452522278\n","Epoch 12 Loss 0.7992134690284729\n","Epoch 13 Loss 0.7237515449523926\n","Epoch 14 Loss 0.6497401595115662\n","Epoch 15 Loss 0.572795569896698\n","Epoch 16 Loss 0.5020315051078796\n","Epoch 17 Loss 0.43824100494384766\n","Epoch 18 Loss 0.38061797618865967\n","Epoch 19 Loss 0.3326880931854248\n","Epoch 20 Loss 0.29584401845932007\n","Epoch 21 Loss 0.26795318722724915\n","Epoch 22 Loss 0.246038019657135\n","Epoch 23 Loss 0.2276441603899002\n","Epoch 24 Loss 0.21077197790145874\n","Epoch 25 Loss 0.1929078847169876\n","Epoch 26 Loss 0.1691320687532425\n","Epoch 27 Loss 0.14251141250133514\n","Epoch 28 Loss 0.13340045511722565\n","Epoch 29 Loss 0.12046491354703903\n","Epoch 30 Loss 0.1084408164024353\n","Epoch 31 Loss 0.10414760559797287\n","Epoch 32 Loss 0.10200699418783188\n","Epoch 33 Loss 0.09624483436346054\n","Epoch 34 Loss 0.08823928236961365\n","Epoch 35 Loss 0.08584722131490707\n","Epoch 36 Loss 0.08382842689752579\n","Epoch 37 Loss 0.07629474997520447\n","Epoch 38 Loss 0.07415112107992172\n","Epoch 39 Loss 0.0730770155787468\n","Epoch 40 Loss 0.07023876905441284\n","Epoch 41 Loss 0.0671762153506279\n","Epoch 42 Loss 0.06601125746965408\n","Epoch 43 Loss 0.06506463885307312\n","Epoch 44 Loss 0.0617644302546978\n","Epoch 45 Loss 0.05873386189341545\n","Epoch 46 Loss 0.05716321989893913\n","Epoch 47 Loss 0.05417966470122337\n","Epoch 48 Loss 0.050615470856428146\n","Epoch 49 Loss 0.04751299321651459\n","Epoch 50 Loss 0.04467805102467537\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9-bnkwPD8kYj","colab_type":"text"},"source":["# Decoding with `ctcdecode`\n","\n","During inference, we want to generate the most probable sequence from predicted probabilities. PyTorch doesn't have built-in support for that, so we need another library called `ctcdecode`.\n","\n","## Installation\n","\n","If you just follow the steps in https://github.com/parlance/ctcdecode, you may encounter `ModuleNotFoundError: No module named 'wget'`. Simply `pip install wget` solves the problem.\n","\n","Installing `ctcdecode` with the following steps should be successful. (Change `pip3 install` to either `pip3 install --user` or `sudo -H pip3 install` if you are using the system Python instead of Conda) It takes a few minutes to compile, so be patient."]},{"cell_type":"code","metadata":{"id":"ywIzDlKp7ppv","colab_type":"code","outputId":"436c5038-a021-40b5-c50a-d8d4d669262d","executionInfo":{"status":"ok","timestamp":1571610786077,"user_tz":240,"elapsed":175627,"user":{"displayName":"Parth Shah","photoUrl":"","userId":"06799060630439446761"}},"colab":{"base_uri":"https://localhost:8080/","height":716}},"source":["!git clone --recursive https://github.com/parlance/ctcdecode.git\n","!pip3 install wget\n","%cd ctcdecode\n","!pip3 install .\n","%cd .."],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'ctcdecode'...\n","remote: Enumerating objects: 1006, done.\u001b[K\n","remote: Total 1006 (delta 0), reused 0 (delta 0), pack-reused 1006\u001b[K\n","Receiving objects: 100% (1006/1006), 728.22 KiB | 1.83 MiB/s, done.\n","Resolving deltas: 100% (500/500), done.\n","Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n","Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n","Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n","remote: Enumerating objects: 82, done.        \n","remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n","Cloning into '/content/ctcdecode/third_party/kenlm'...\n","remote: Enumerating objects: 5, done.        \n","remote: Counting objects: 100% (5/5), done.        \n","remote: Compressing objects: 100% (5/5), done.        \n","remote: Total 13329 (delta 0), reused 1 (delta 0), pack-reused 13324        \n","Receiving objects: 100% (13329/13329), 5.33 MiB | 7.56 MiB/s, done.\n","Resolving deltas: 100% (7645/7645), done.\n","Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n","Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n","Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=ed38680c17f8e8fa8d7d14b475fc866be2579335bb37981fcacc8644bc855bfb\n","  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","/content/ctcdecode\n","Processing /content/ctcdecode\n","Building wheels for collected packages: ctcdecode\n","  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ctcdecode: filename=ctcdecode-0.4-cp36-cp36m-linux_x86_64.whl size=11831481 sha256=c05dc6323636817bbe8acd3f1527600d0d95929842ea139a960879af2a430988\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5n8e90yy/wheels/c3/6c/94/7d57d4f20a87a22ef1722eaad22052b4c435892b55400e5f4e\n","Successfully built ctcdecode\n","Installing collected packages: ctcdecode\n","Successfully installed ctcdecode-0.4\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S8V_A9NF9A0H","colab_type":"text"},"source":["Test whether ctcdecode is working.\n","\n","Common errors:\n","* `ImportError: No module named 'ctcdecode._ext'`: Your current working directory is in `ctcdecode`. `cd` into other directories will solve this.\n","* `undefined symbol: _ZN6caffe26detail37_typeMetaDataInstance_preallocated_32E`: **`torch` MUST be imported before importing `ctcdecode`**, otherwise you will see this.\n"]},{"cell_type":"code","metadata":{"id":"TYpExO0A_cJD","colab_type":"code","outputId":"59e73cb7-a4fe-492b-ea39-44f10e7e8efe","executionInfo":{"status":"ok","timestamp":1571618510472,"user_tz":240,"elapsed":486,"user":{"displayName":"Parth Shah","photoUrl":"","userId":"06799060630439446761"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import torch\n","from ctcdecode import CTCBeamDecoder\n","\n","decoder = CTCBeamDecoder([' ', 'A'], beam_width=4)\n","probs = torch.Tensor([[0.2, 0.8], [0.8, 0.2]]).unsqueeze(0)\n","print(probs.size())\n","out, _, _, out_lens = decoder.decode(probs, torch.LongTensor([2]))\n","print(out[0, 0, :out_lens[0, 0]])"],"execution_count":1,"outputs":[{"output_type":"stream","text":["torch.Size([1, 2, 2])\n","tensor([1], dtype=torch.int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FqOM2imqROQ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2ce1b722-a19f-4ce8-f2a5-7b9357d5f732","executionInfo":{"status":"ok","timestamp":1571619459477,"user_tz":240,"elapsed":609,"user":{"displayName":"Parth Shah","photoUrl":"","userId":"06799060630439446761"}}},"source":["import numpy as np\n","y_s = np.load('table_of_ys_brand_new.npy')\n","#y_s = np.array([[1/6,4/6,2/6,1/6],[2/6,1/6,1/6,4/6],[3/6,1/6,3/6,1/6]])\n","#y_s = y_s.reshape(y_s.shape[0],y_s.shape[1],1)\n","y_sT = np.transpose(y_s, (2,1,0))\n","tensor_y = torch.Tensor(y_sT)\n","decoder = CTCBeamDecoder([' ','a','b','c'], beam_width=2)\n","out, _, _, out_lens = decoder.decode(tensor_y, torch.LongTensor([10]))\n","print(out[0, 0, :out_lens[0, 0]])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor([1, 2, 2, 3], dtype=torch.int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pODYer9PEb8P","colab_type":"text"},"source":["## Usage\n","\n","There is no documentation for `ctcdecode`. The only definitivly way to understand it is to read the source code. Below we explain some arguments that are more useful.\n","\n","`CTCBeamDecoder`:\n","* `phonemes`: **It doesn't need to be actual phonemes.** The only requirement is being a list of characters whose length is the number of classes (number of phonemes plus 1). \n","* `beam_width`: Larger beam width produces better output, but also costs more time and memory.\n","* `num_processes`: Number of processes for parallel decoding. Setting it to `os.cpu_count()` is recommended as it utilizes all CPU cores.\n","* `log_probs_input`: Should always be True, since your model output is log probabilities.\n","\n","`CTCBeamDecoder.decode` arguments:\n","* `probs`: Prediction from your model as log probabilities (if `log_probs_input=True`).\n","  * Shape: (N, T, C). where N is batch size, T is the largest length in the batch, and C is number of classes. **WARNING!** This dimension order is unconventional in PyTorch. You likely need to do `out.transpose(0, 1)` on your output.\n","* `len`: Lengths of sequences in `probs`.\n","  * Shape: (T,)\n","\n","\n","`CTCBeamDecoder.decode` return value (tuple of 4):\n","* First item `output`: Decoded top sequences.\n","  * Shape: (N, B, T), where B is the beam width. Normally we only need th best sequences, which are indexed 0 in the second (beam width) dimension.\n","* Second and third can be ignored.\n","* Last item `out_seq_len`: Length of sequences in `output`. \n","  * Shape: (N, B). Lengths of best sequences are indexed 0 in the second (beam width) dimension."]},{"cell_type":"code","metadata":{"id":"xcdt3FgdA4OH","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Visualize the probability prediction at each step\n","def visualize(word, log_probs):\n","    fig, ax = plt.subplots()\n","    ax.imshow(log_probs.exp().numpy())\n","    ax.set_xticks(np.arange(log_probs.size(1)))\n","    ax.set_yticks(np.arange(log_probs.size(0)))\n","    ax.set_xticklabels(phonemes)\n","    ax.set_yticklabels(list(word))\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9wgxDz-EYhn","colab_type":"code","outputId":"38f08706-d9d3-4a96-ff57-cc49b675e3a9","executionInfo":{"status":"ok","timestamp":1571405174305,"user_tz":240,"elapsed":1117,"user":{"displayName":"Liwei Cai","photoUrl":"","userId":"16466533746106471957"}},"colab":{"base_uri":"https://localhost:8080/","height":869}},"source":["test_data = ['TEE', 'TINT', 'SINE', 'SENT']\n","\n","decoder = CTCBeamDecoder(['$'] * len(phonemes), beam_width=4, log_probs_input=True)\n","\n","test_X = [torch.LongTensor([letters.find(c) for c in word]) for word in test_data]\n","test_X_lens = torch.LongTensor([len(seq) for seq in test_X])\n","test_X = pad_sequence(test_X)\n","\n","with torch.no_grad():\n","    out, out_lens = model(test_X, test_X_lens)\n","\n","test_Y, _, _, test_Y_lens = decoder.decode(out.transpose(0, 1), out_lens)\n","for i in range(len(test_data)):\n","    visualize(test_data[i], out[:len(test_data[i]), i, :])\n","    # For the i-th sample in the batch, get the best output\n","    best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]\n","    best_pron = ' '.join(phonemes[i] for i in best_seq)\n","    print(test_data[i], '->', best_pron)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW0AAACgCAYAAADHAD38AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["TEE -> T IY\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW8AAADKCAYAAABqmsH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACE5JREFUeJzt3U+opQd5x/HfYzOZMQkKJkIaFEUR\nRSoM9WJQ2oK6cFOwiAuDoAthcCUR4h9Q2gtdWEpC7UKhQ6FWEYxE3HShEksEQZFEhvgHiYgEMW1J\nYiCOpGNNni7utZymM3PP2Pfkvc/4+Wxyz8nLmx85k2/eee+5c6q7A8Asz1t7AABXTrwBBhJvgIHE\nG2Ag8QYYSLwBBhJvgIHEG2Ag8QYY6JpdnfjaOtmncv2uTr+YCy85/huTJEN+EPbkz3+19oSrR9Xa\nC64uQ36a/Jd54rHufvFRx+0s3qdyfW6tt+7q9Iv5ye1vXHvCVuqZtRds5xUf+dbaE64adeLatSds\n53kz/ifTFy6sPWEr9/Y9D29znNsmAAOJN8BA4g0wkHgDDCTeAAOJN8BA4g0wkHgDDCTeAAOJN8BA\n4g0wkHgDDCTeAAOJN8BA4g0wkHgDDLTVhzFU1Y1Jvn748OYkTyd59PDxG7r71zvYBsAlbBXv7n48\nyekkqar9JOe7+84d7gLgMtw2ARho0c+wrKozSc4kyalct+SpAdiw6JV3d5/t7r3u3juRk0ueGoAN\nbpsADCTeAAOJN8BAV/wNy+7e38EOAK6AK2+AgcQbYCDxBhhIvAEGEm+AgcQbYCDxBhhIvAEGEm+A\ngcQbYCDxBhhIvAEGEm+AgcQbYCDxBhho0Q8gnuiVH/722hO20732Ap5jD/3jH609YSuveu93156w\nlZ99/E1rT9jOX9+z1WGuvAEGEm+AgcQbYCDxBhhIvAEGEm+AgcQbYCDxBhhIvAEGEm+AgcQbYCDx\nBhhIvAEGEm+AgcQbYCDxBhjod4p3VZ1feggA23PlDTCQeAMMJN4AAy36AcRVdSbJmSQ5leuWPDUA\nGxa98u7us9291917J3JyyVMDsMFtE4CBxBtgoN8p3t19w9JDANieK2+AgcQbYCDxBhhIvAEGEm+A\ngcQbYCDxBhhIvAEGEm+AgcQbYCDxBhhIvAEGEm+AgcQbYCDxBhhIvAEGqu7eyYlfUC/qW+utOzk3\nx9dXHzm39oQjve2W02tPgEu6t+95oLv3jjrOlTfAQOINMJB4Awwk3gADiTfAQOINMJB4Awwk3gAD\niTfAQOINMJB4Awwk3gADiTfAQOINMJB4Awwk3gADHRnvquqqumvj8R1Vtb/TVQBc1jZX3heSvKOq\nbtr1GAC2s028f5PkbJIP7ngLAFva9p73p5K8u6peuMsxAGxnq3h395NJPpvkA5c7rqrOVNX9VXX/\nf+XCEvsAuIgrebfJJ5O8L8n1lzqgu8929153753Iyf/3OAAubut4d/cvknwxBwEHYEVX+j7vu5J4\n1wnAyq456oDuvmHj6/9Ict1OFwFwJD9hCTCQeAMMJN4AA4k3wEDiDTCQeAMMJN4AA4k3wEDiDTCQ\neAMMJN4AA4k3wEDiDTCQeAMMJN4AA4k3wEBHfhgDx8NXHzm39oStvO2W02tPgIurWnvBdnq7w1x5\nAwwk3gADiTfAQOINMJB4Awwk3gADiTfAQOINMJB4Awwk3gADiTfAQOINMJB4Awwk3gADiTfAQOIN\nMNBWH8ZQVTcm+frhw5uTPJ3k0cPHb+juX+9gGwCXsFW8u/vxJKeTpKr2k5zv7jt3uAuAy3DbBGAg\n8QYYaNEPIK6qM0nOJMmpXLfkqQHYsOiVd3ef7e697t47kZNLnhqADW6bAAwk3gADXfE97+7e38EO\nAK6AK2+AgcQbYCDxBhhIvAEGEm+AgcQbYCDxBhhIvAEGEm+AgcQbYCDxBhhIvAEGEm+AgcQbYCDx\nBhhIvAEGqu7ezYmrHk3y8MKnvSnJYwufc2kTNiZ2Ls3OZf0+73xZd7/4qIN2Fu9dqKr7u3tv7R2X\nM2FjYufS7FyWnUdz2wRgIPEGGGhavM+uPWALEzYmdi7NzmXZeYRR97wBODDtyhuAiPeiqupjVfWD\nqnqwqs5V1a1rb9pUVTce7jpXVf9eVT/feHzt2vt+q6q6qu7aeHxHVe2vOOmiqup8VZ2qqh9V1es2\nnv9QVf3Dmtt+q6rOH/715VX1/Wf9vf2qumOdZf+z4emNX4Pnquqjh8/fV1V7G8f9n/3Ptar6i8Nf\nm685Dq/7Nc/VP+hqV1VvTPLnSf64uy9U1U1Jjk0Qk6S7H09yOjn4DzfJ+e6+c9VRF3chyTuq6hPd\nfazf69vd/1lVtyf5dFX9WZJbkrw/ybF/m9sx8VR3n157xJZuS/LNJLd191+t/bq78l7OHyZ5rLsv\nJEl3P9bdj6y8aarf5OAbQR9ce8g2uvsrSf4tyXuS/F2S/e5+Yt1VLKmqbkjyJ0nel+Rdyfqvuyvv\n5XwtyV9W1UNJ7k1yd3d/Y+VNk30qyYNV9bdrD9nS7Um+k+TH3f25tcdcwiur6tzG45uTrP07r+c/\na9Mnuvvuw68/X1VPHX59bZJnnttp/8vbk3ylux+qqser6vXd/UBWfN3FeyHdfb6qXp/kT5O8Ocnd\nVfXR7v7Mustm6u4nq+qzST6Q5Kmjjl9bdz9SVf+a5F/W3nIZP9m8RXFMvo9wudsm7+7u+5ODe95Z\n99/tbUn+/vDrLxw+fmDN1128F9TdTye5L8l9VfW9JO9N8pk1Nw33ySTfTfJPaw/Z0jNZ9+qQHaiq\nFyV5S5LXVVUn+YMkXVUf6oP3Wq/yurvnvZCqenVVvWrjqdNZ/g/m+r3S3b9I8sUc3GeEtbwzyee6\n+2Xd/fLufmmSn+bgd9mrEe/l3JDkn6vqh1X1YJLXJtlfd9JV4a4c/MltXJ2e/6y3Cv7N2oMu4rYk\nX37Wc186fH41fsISYCBX3gADiTfAQOINMJB4Awwk3gADiTfAQOINMJB4Awz035NdzKqt8bGnAAAA\nAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["TINT -> S IH N T\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW8AAADKCAYAAABqmsH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["SINE -> S AY N\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW8AAADKCAYAAABqmsH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["SENT -> S EH N T\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GEWfU3tIEP7K","colab_type":"text"},"source":["## Caveats\n","\n","* Your program will **crash sliently** if you provide invalid arguments to `CTCBeamDecoder.decode`, like having wrong shapes. It is very difficult to debug such error. During development, we recommend you to **print out all arguments before decoding**, so that you can figure out what goes wrong if it crashes."]},{"cell_type":"code","metadata":{"id":"p2oLo8ZfEg-9","colab_type":"code","colab":{}},"source":["# Don't run this! It will crash your notebook.\n","decoder = CTCBeamDecoder([' ', 'A'], beam_width=4)\n","probs = torch.Tensor([[0.1, 0.1, 0.8], [0.8, 0.1, 0.1]]).unsqueeze(0)\n","out, _, _, out_lens = decoder.decode(probs, torch.LongTensor([2]))"],"execution_count":0,"outputs":[]}]}