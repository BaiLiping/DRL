{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a python framework for machine learning\n",
    "\n",
    "- GPU-accelerated computations\n",
    "- automatic differentiation\n",
    "- modules for neural networks\n",
    "\n",
    "This tutorial will teach you the fundamentals of operating on pytorch tensors and networks. You have already seen some things in recitation 0 which we will quickly review, but most of this tutorial is on mostly new or more advanced stuff.\n",
    "\n",
    "For a worked example of how to build and train a pytorch network, see `pytorch-example.ipynb`.\n",
    "\n",
    "For additional tutorials, see http://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are the fundamental object for array data. The most common types you will use are `IntTensor` and `FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3911e-05, 1.0081e-08, 2.0990e-07],\n",
      "        [2.1085e-07, 4.0523e-11, 7.1450e+31]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create uninitialized tensor\n",
    "x = torch.FloatTensor(2,3)\n",
    "print(x)\n",
    "# Initialize to zeros\n",
    "x.zero_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6965, 0.2861, 0.2269],\n",
      "        [0.5513, 0.7195, 0.4231]])\n",
      "tensor([[0.6965, 0.2861, 0.2269],\n",
      "        [0.5513, 0.7195, 0.4231]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create from numpy array (seed for repeatability)\n",
    "np.random.seed(123)\n",
    "np_array = np.random.random((2,3))\n",
    "print(torch.FloatTensor(np_array))\n",
    "print(torch.from_numpy(np_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696],\n",
      "        [-0.2404, -1.1969,  0.2093]])\n",
      "[[-0.11146712  0.12036294 -0.3696345 ]\n",
      " [-0.24041797 -1.1969243   0.20926936]]\n"
     ]
    }
   ],
   "source": [
    "# Create random tensor (seed for repeatability)\n",
    "torch.manual_seed(123)\n",
    "x=torch.randn(2,3)\n",
    "print(x)\n",
    "# export to numpy array\n",
    "x_np = x.numpy()\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# special tensors (see documentation)\n",
    "print(torch.eye(3))\n",
    "print(torch.ones(2,3))\n",
    "print(torch.zeros(2,3))\n",
    "print(torch.arange(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tensors have a `size` and `type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x=torch.FloatTensor(3,4)\n",
    "print(x.size())\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math, Linear Algebra, and Indexing (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch math and linear algebra is similar to numpy. Operators are overridden so you can use standard math operators (`+`,`-`, etc.) and expect a tensor as a result. See pytorch documentation for a complete list of available functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(85.7910)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0.,5.)\n",
    "print(torch.sum(x))\n",
    "print(torch.sum(torch.exp(x)))\n",
    "print(torch.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch indexing is similar to numpy indexing. See pytorch documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n",
      "tensor([0.3164, 0.4017])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,2)\n",
    "print(x)\n",
    "print(x[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be copied between CPU and GPU. It is important that everything involved in a calculation is on the same device. \n",
    "\n",
    "This portion of the tutorial may not work for you if you do not have a GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1a729973c03d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# cannot get GPU tensor as numpy array directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# create a tensor\n",
    "x = torch.rand(3,2)\n",
    "# copy to GPU\n",
    "y = x.cuda()\n",
    "# copy back to CPU\n",
    "z = y.cpu()\n",
    "# get CPU tensor as numpy array\n",
    "# cannot get GPU tensor as numpy array directly\n",
    "try:\n",
    "    y.numpy()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations between GPU and CPU tensors will fail. Operations require all arguments to be on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,5)  # CPU tensor\n",
    "y = torch.rand(5,4).cuda()  # GPU tensor\n",
    "try:\n",
    "    torch.mm(x,y)  # Operation between CPU and GPU fails\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical code should include `if` statements or utilize helper functions so it can operate with or without the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2745, 0.6584],\n",
      "        [0.2775, 0.8573],\n",
      "        [0.8993, 0.0390]], device='cuda:0') torch.float32\n",
      "tensor([[0.0753, 0.4335],\n",
      "        [0.0770, 0.7350],\n",
      "        [0.8088, 0.0015]], device='cuda:0')\n",
      "tensor([[0.0753, 0.4335],\n",
      "        [0.0770, 0.7350],\n",
      "        [0.8088, 0.0015]]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Put tensor on CUDA if available\n",
    "x = torch.rand(3,2)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    print(x, x.dtype)\n",
    "    \n",
    "# Do some calculations\n",
    "y = x ** 2 \n",
    "print(y)\n",
    "\n",
    "# Copy to CPU if on GPU\n",
    "if y.is_cuda:\n",
    "    y = y.cpu()\n",
    "    print(y, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient method is `new`, which creates a new tensor on the same device as another tensor. It should be used for creating tensors whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n",
      "tensor([[0.0753, 0.4335]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(3,2)\n",
    "x2 = x1.new(1,2)  # create cpu tensor\n",
    "print(x2)\n",
    "x1 = torch.rand(3,2).cuda()\n",
    "x2 = x1.new(1,2)  # create cuda tensor\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations executed on the GPU can be many times faster than numpy. However, numpy is still optimized for the CPU and many times faster than python `for` loops. Numpy calculations may be faster than GPU calculations for small arrays due to the cost of interfacing with the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "# Create random data\n",
    "x = torch.rand(1000,64)\n",
    "y = torch.rand(64,32)\n",
    "number = 10000  # number of iterations\n",
    "\n",
    "def square():\n",
    "    z=torch.mm(x, y) # dot product (mm=matrix multiplication)\n",
    "\n",
    "# Time CPU\n",
    "print('CPU: {}ms'.format(timeit(square, number=number)*1000))\n",
    "# Time GPU\n",
    "x, y = x.cuda(), y.cuda()\n",
    "print('GPU: {}ms'.format(timeit(square, number=number)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors provide automatic differentiation.\n",
    "\n",
    "As you might know, previous versions of Pytorch used Variables, which were wrappers around tensors for differentiation. Starting with pytorch 0.4.0, this wrapping is done internally in the Tensor class and you can, and should, differentiate Tensors directly. However, it is possible that you walk on references to Variables, e.g. in your error messages.\n",
    "\n",
    "What you need to remember :\n",
    "\n",
    "- Tensors you are differentiating with respect to must have `requires_grad=True`\n",
    "- Call `.backward()` on scalar variables you are differentiating\n",
    "- To differentiate a vector, sum it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-aabc06498b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Calculate gradient (dy/dx=2x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# Print values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Create differentiable tensor\n",
    "x = torch.tensor(torch.arange(0,4), requires_grad=False)\n",
    "print(x.dtype)\n",
    "# Calculate y=sum(x**2)\n",
    "y = x**2\n",
    "# Calculate gradient (dy/dx=2x)\n",
    "y.sum().backward()\n",
    "# Print values\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiation accumulates gradients. This is sometimes what you want and sometimes not. **Make sure to zero gradients between batches if performing gradient descent or you will get strange results!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3546ec580fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create a variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Differentiate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "# Create a variable\n",
    "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "# Differentiate\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Differentiate again (accumulates gradient)\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Zero gradient before differentiating\n",
    "x.grad.data.zero_()\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a Tensor with gradient cannot be exported to numpy directly :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8c8213f31fb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# raises an exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "x.numpy() # raises an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that pytorch remembers the graph of all computations to perform differenciation. To be integrated to this graph the raw data is wrapped internally to the Tensor class (like what was formerly a Variable). You can detach the tensor from the graph using the **.detach()** method, which returns a tensor with the same data but requires_grad set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-73751dbe633c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "y=x**2\n",
    "z=y**2\n",
    "z.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reason to use this method is that updating the graph can use a lot of memory. If you are in a context where you have a differentiable tensor that you don't need to differentiate, think of detaching it from the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch provides a framework for developing neural network modules. They take care of many things, the main one being wrapping and tracking a list of parameters for you.\n",
    "You have several ways of building and using a network, offering different tradeoffs between freedom and simplicity.\n",
    "\n",
    "torch.nn provides basic 1-layer nets, such as Linear (perceptron) and activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-852be3b3b478>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,32)\n",
    "net = torch.nn.Linear(32,10)\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All nn.Module objects are reusable as components of bigger networks ! That is how you build personnalized nets. The simplest way is to use the nn.Sequential class.\n",
    "\n",
    "You can also create your own class that inherits n.Module. The forward method should precise what happens in the forward pass given an input. This enables you to precise behaviors more complicated than just applying layers one after another, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple sequential network (`nn.Module` object) from layers (other `nn.Module` objects).\n",
    "# Here a MLP with 2 layers and sigmoid activation.\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(32,128),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(128,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a more customizable network module (equivalent here)\n",
    "class MyNetwork(torch.nn.Module):\n",
    "    # you can use the layer sizes as initialization arguments if you want to\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
    "        self.layer2 = torch.nn.Sigmoid()\n",
    "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, input_val):\n",
    "        h = input_val\n",
    "        h = self.layer1(h)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        return h\n",
    "\n",
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network tracks parameters, and you can access them through the **parameters()** method, which returns a python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0154,  0.0785, -0.1184,  ...,  0.1257,  0.0365,  0.0124],\n",
      "        [-0.0234, -0.0679,  0.1513,  ..., -0.1436, -0.0187,  0.1284],\n",
      "        [-0.1456,  0.0865,  0.1176,  ...,  0.1445,  0.0372, -0.0647],\n",
      "        ...,\n",
      "        [-0.1480, -0.1639,  0.1487,  ...,  0.0847, -0.0920,  0.1166],\n",
      "        [-0.1258, -0.1310,  0.0416,  ...,  0.0714, -0.0716,  0.1599],\n",
      "        [ 0.1268,  0.0177, -0.1755,  ..., -0.1349,  0.0671, -0.0431]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0408, -0.0123, -0.1635,  0.0330,  0.1358, -0.1656, -0.0614, -0.0220,\n",
      "        -0.0578,  0.0027,  0.0237,  0.0159,  0.1424,  0.1321, -0.1763,  0.0188,\n",
      "         0.0380, -0.0411, -0.1350, -0.1282, -0.1592, -0.0935,  0.1409,  0.1473,\n",
      "         0.0075,  0.0541, -0.0385,  0.0188,  0.1385, -0.1126, -0.0476,  0.1354,\n",
      "        -0.0513, -0.0271, -0.0602,  0.1677,  0.0115, -0.0710, -0.0480,  0.0504,\n",
      "         0.0364,  0.1576, -0.0507,  0.1083, -0.1162, -0.0565, -0.0122,  0.0131,\n",
      "         0.0323, -0.1086,  0.1145,  0.1153,  0.1578, -0.1741, -0.0879, -0.1259,\n",
      "         0.1038, -0.1328,  0.1236,  0.0834,  0.1593,  0.1020, -0.1573, -0.0994,\n",
      "        -0.0793,  0.0433, -0.0659, -0.0446, -0.1097, -0.0849, -0.1697, -0.1460,\n",
      "         0.1269,  0.0598,  0.0937,  0.1518, -0.1610,  0.0869, -0.1608, -0.1033,\n",
      "         0.0641, -0.0692, -0.1414,  0.1670, -0.0994,  0.0192, -0.0146, -0.0799,\n",
      "        -0.0333,  0.1528, -0.0130,  0.1032,  0.1355, -0.1415, -0.1337,  0.0166,\n",
      "         0.1482,  0.1602, -0.0959, -0.0823,  0.1096,  0.0286, -0.1467,  0.0226,\n",
      "        -0.1740,  0.1762,  0.1307,  0.0437,  0.0863, -0.1170,  0.0367, -0.1560,\n",
      "         0.0085, -0.0489, -0.1354,  0.1188, -0.1400, -0.1756, -0.1462,  0.0477,\n",
      "         0.0354,  0.1381, -0.1254,  0.0803, -0.1199, -0.1253,  0.1687,  0.1177],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0066, -0.0808, -0.0834,  ...,  0.0266,  0.0193, -0.0197],\n",
      "        [ 0.0808,  0.0397,  0.0068,  ...,  0.0678, -0.0145, -0.0704],\n",
      "        [ 0.0497, -0.0005, -0.0710,  ...,  0.0769,  0.0083, -0.0255],\n",
      "        ...,\n",
      "        [ 0.0102,  0.0531,  0.0660,  ...,  0.0396,  0.0655, -0.0493],\n",
      "        [-0.0113,  0.0194,  0.0057,  ...,  0.0566,  0.0331,  0.0257],\n",
      "        [ 0.0370, -0.0362,  0.0160,  ...,  0.0644, -0.0816,  0.0409]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0840,  0.0421, -0.0007,  0.0452,  0.0301, -0.0507, -0.0267,  0.0139,\n",
      "        -0.0498,  0.0497], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are of type Parameter, which is basically a wrapper for a tensor. How does pytorch retrieve your network's parameters ? They are simply all the attributes of type Parameter in your network. Moreover, if an attribute is of type nn.Module, its own parameters are added to your network's parameters ! This is why, when you define a network by adding up basic components such as nn.Linear, you should never have to explicitely define parameters.\n",
    "\n",
    "However, if you are in a case where no pytorch default module does what you need, you can define parameters explicitely (this should be rare). For the record, let's build the previous MLP with personnalized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetworkWithParams(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(MyNetworkWithParams,self).__init__()\n",
    "        self.layer1_weights = nn.Parameter(torch.randn(input_size,hidden_size))\n",
    "        self.layer1_bias = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.layer2_weights = nn.Parameter(torch.randn(hidden_size,output_size))\n",
    "        self.layer2_bias = nn.Parameter(torch.randn(output_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h1 = torch.matmul(x,self.layer1_weights) + self.layer1_bias\n",
    "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
    "        output = torch.matmul(h1_act,self.layer2_weights) + self.layer2_bias\n",
    "        return output\n",
    "\n",
    "net = MyNetworkWithParams(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are useful in that they are meant to be all the network's weights that will be optimized during training. If you were needing to use a tensor in your computational graph that you want to remain constant, just define it as a regular tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nn.Module also provides loss functions, such as cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1876, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n",
    "y = torch.tensor([0,3,9])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.CrossEntropyLoss does both the softmax and the actual cross-entropy : given $output$ of size $(n,d)$ and $y$ of size $n$ and values in $0,1,...,d-1$, it computes $\\sum_{i=0}^{n-1}log(s[i,y[i]])$ where $s[i,j] = \\frac{e^{output[i,j]}}{\\sum_{j'=0}^{d-1}e^{output[i,j']}}$\n",
    "\n",
    "You can also compose nn.LogSoftmax and nn.NLLLoss to get the same result. Note that all these use the log-softmax rather than the softmax, for stability in the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.1876, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent\n",
    "criterion2 = nn.NLLLoss()\n",
    "sf = nn.LogSoftmax()\n",
    "output = net(x)\n",
    "loss = criterion(sf(output),y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to perform the backward pass, just execute **loss.backward()** ! It will update gradients in all differentiable tensors in the graph, which in particular includes all the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.9255e-03, -5.9252e-03, -5.9249e-03,  ..., -5.9165e-03,\n",
      "         -5.9162e-03, -5.9159e-03],\n",
      "        [ 2.0501e-03,  2.0442e-03,  2.0384e-03,  ...,  1.8805e-03,\n",
      "          1.8746e-03,  1.8688e-03],\n",
      "        [ 2.3427e-03,  2.3427e-03,  2.3427e-03,  ...,  2.3427e-03,\n",
      "          2.3427e-03,  2.3427e-03],\n",
      "        ...,\n",
      "        [ 7.0074e-04,  3.5227e-04,  3.8067e-06,  ..., -9.4048e-03,\n",
      "         -9.7533e-03, -1.0102e-02],\n",
      "        [ 2.5619e-03,  3.3242e-03,  4.0865e-03,  ...,  2.4669e-02,\n",
      "          2.5431e-02,  2.6193e-02],\n",
      "        [-3.4918e-03, -3.4963e-03, -3.5009e-03,  ..., -3.6227e-03,\n",
      "         -3.6272e-03, -3.6317e-03]])\n",
      "tensor([-2.1000e-03, -2.4149e-03, -4.7471e-03,  1.8195e-03, -3.0208e-03,\n",
      "        -1.5004e-03,  1.7470e-03,  4.4897e-03, -4.6868e-03, -4.3105e-03,\n",
      "         9.4013e-04, -8.7316e-04, -2.6598e-03,  1.8245e-03,  6.0240e-04,\n",
      "        -3.3299e-03, -7.6377e-03,  9.3953e-03,  7.7175e-03,  6.4383e-03,\n",
      "        -1.9766e-03,  4.1844e-03,  1.1237e-02, -7.3580e-03, -6.6197e-03,\n",
      "        -1.5482e-03, -6.2764e-03,  4.1583e-03, -6.3827e-04,  2.7127e-03,\n",
      "        -1.1002e-02,  1.6643e-03,  7.6189e-03,  6.0585e-03,  3.1560e-03,\n",
      "        -6.9747e-05, -6.4153e-03, -1.0988e-02,  5.0511e-03, -5.0119e-03,\n",
      "         7.9282e-03,  2.1348e-03,  2.8302e-03, -7.8369e-03, -3.1542e-03,\n",
      "        -9.1892e-05, -7.3795e-03, -2.1413e-03,  2.0942e-03,  5.8748e-04,\n",
      "         4.5367e-03, -6.0247e-03, -1.3525e-02, -7.6022e-03,  6.7013e-04,\n",
      "         6.1592e-03,  7.7162e-03, -3.4325e-04, -1.8184e-03,  1.5441e-03,\n",
      "        -4.0974e-03, -2.0951e-03,  3.8093e-05,  1.3242e-03,  1.6624e-03,\n",
      "        -3.6945e-03, -5.0166e-03,  6.0936e-04, -5.4959e-03,  2.6684e-03,\n",
      "        -1.1182e-03,  3.3520e-03,  4.9249e-03,  3.3509e-03,  2.4671e-03,\n",
      "        -3.8285e-03, -1.5653e-03,  1.7803e-03, -1.6577e-04,  6.6400e-03,\n",
      "        -8.3083e-04,  4.4862e-03, -3.3887e-03,  3.2356e-03, -9.7528e-03,\n",
      "         1.0786e-02,  2.2551e-03,  3.8904e-03,  3.9129e-04,  9.4325e-04,\n",
      "        -6.1745e-03, -5.5443e-03, -2.7359e-03,  1.0981e-03,  5.1030e-04,\n",
      "        -2.1313e-03,  7.6526e-03,  3.8139e-03, -9.5370e-03,  1.8283e-04,\n",
      "         9.5392e-03,  7.1301e-03,  3.1314e-03, -7.0155e-04,  7.8991e-04,\n",
      "        -8.9502e-04,  5.6708e-03,  3.9138e-03,  7.6671e-03, -5.4279e-03,\n",
      "        -2.4635e-03,  1.2210e-02,  7.9176e-03,  7.7904e-03, -1.1618e-02,\n",
      "         1.8521e-04,  4.2403e-03,  3.6953e-03,  1.6357e-03,  6.9411e-03,\n",
      "        -1.2803e-03,  1.0819e-02, -5.7625e-03,  1.1821e-02, -4.6331e-03,\n",
      "        -9.9568e-04,  5.2922e-03, -2.0095e-03])\n",
      "tensor([[-0.2133,  0.0418,  0.0360,  ..., -0.2181,  0.0240, -0.2224],\n",
      "        [ 0.1020,  0.0372,  0.0317,  ...,  0.0935,  0.0413,  0.0939],\n",
      "        [ 0.0599,  0.0254,  0.0218,  ...,  0.0544,  0.0275,  0.0545],\n",
      "        ...,\n",
      "        [ 0.0572,  0.0248,  0.0214,  ...,  0.0520,  0.0266,  0.0520],\n",
      "        [ 0.0502,  0.0182,  0.0155,  ...,  0.0460,  0.0203,  0.0462],\n",
      "        [-0.1426, -0.0728, -0.0377,  ..., -0.1011, -0.0952, -0.1119]])\n",
      "tensor([-0.1717,  0.1384,  0.0848, -0.2503,  0.0821,  0.0877,  0.1065,  0.0814,\n",
      "         0.0681, -0.2270])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "# Check that the parameters now have gradients\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1851e-02, -1.1850e-02, -1.1850e-02,  ..., -1.1833e-02,\n",
      "         -1.1832e-02, -1.1832e-02],\n",
      "        [ 4.1001e-03,  4.0884e-03,  4.0767e-03,  ...,  3.7609e-03,\n",
      "          3.7492e-03,  3.7375e-03],\n",
      "        [ 4.6854e-03,  4.6854e-03,  4.6854e-03,  ...,  4.6854e-03,\n",
      "          4.6854e-03,  4.6854e-03],\n",
      "        ...,\n",
      "        [ 1.4015e-03,  7.0455e-04,  7.6134e-06,  ..., -1.8810e-02,\n",
      "         -1.9507e-02, -2.0204e-02],\n",
      "        [ 5.1237e-03,  6.6483e-03,  8.1729e-03,  ...,  4.9337e-02,\n",
      "          5.0862e-02,  5.2386e-02],\n",
      "        [-6.9837e-03, -6.9927e-03, -7.0017e-03,  ..., -7.2454e-03,\n",
      "         -7.2544e-03, -7.2635e-03]])\n",
      "tensor([-4.2001e-03, -4.8299e-03, -9.4943e-03,  3.6389e-03, -6.0416e-03,\n",
      "        -3.0008e-03,  3.4940e-03,  8.9794e-03, -9.3735e-03, -8.6211e-03,\n",
      "         1.8803e-03, -1.7463e-03, -5.3195e-03,  3.6490e-03,  1.2048e-03,\n",
      "        -6.6598e-03, -1.5275e-02,  1.8791e-02,  1.5435e-02,  1.2877e-02,\n",
      "        -3.9532e-03,  8.3688e-03,  2.2474e-02, -1.4716e-02, -1.3239e-02,\n",
      "        -3.0964e-03, -1.2553e-02,  8.3165e-03, -1.2765e-03,  5.4254e-03,\n",
      "        -2.2004e-02,  3.3286e-03,  1.5238e-02,  1.2117e-02,  6.3119e-03,\n",
      "        -1.3949e-04, -1.2831e-02, -2.1976e-02,  1.0102e-02, -1.0024e-02,\n",
      "         1.5856e-02,  4.2695e-03,  5.6604e-03, -1.5674e-02, -6.3084e-03,\n",
      "        -1.8378e-04, -1.4759e-02, -4.2826e-03,  4.1883e-03,  1.1750e-03,\n",
      "         9.0735e-03, -1.2049e-02, -2.7049e-02, -1.5204e-02,  1.3403e-03,\n",
      "         1.2318e-02,  1.5432e-02, -6.8649e-04, -3.6368e-03,  3.0883e-03,\n",
      "        -8.1948e-03, -4.1901e-03,  7.6186e-05,  2.6483e-03,  3.3248e-03,\n",
      "        -7.3891e-03, -1.0033e-02,  1.2187e-03, -1.0992e-02,  5.3369e-03,\n",
      "        -2.2364e-03,  6.7040e-03,  9.8498e-03,  6.7018e-03,  4.9342e-03,\n",
      "        -7.6570e-03, -3.1307e-03,  3.5606e-03, -3.3153e-04,  1.3280e-02,\n",
      "        -1.6617e-03,  8.9724e-03, -6.7774e-03,  6.4711e-03, -1.9506e-02,\n",
      "         2.1572e-02,  4.5102e-03,  7.7807e-03,  7.8259e-04,  1.8865e-03,\n",
      "        -1.2349e-02, -1.1089e-02, -5.4718e-03,  2.1962e-03,  1.0206e-03,\n",
      "        -4.2626e-03,  1.5305e-02,  7.6277e-03, -1.9074e-02,  3.6567e-04,\n",
      "         1.9078e-02,  1.4260e-02,  6.2627e-03, -1.4031e-03,  1.5798e-03,\n",
      "        -1.7900e-03,  1.1342e-02,  7.8276e-03,  1.5334e-02, -1.0856e-02,\n",
      "        -4.9270e-03,  2.4420e-02,  1.5835e-02,  1.5581e-02, -2.3236e-02,\n",
      "         3.7041e-04,  8.4806e-03,  7.3907e-03,  3.2713e-03,  1.3882e-02,\n",
      "        -2.5607e-03,  2.1639e-02, -1.1525e-02,  2.3643e-02, -9.2661e-03,\n",
      "        -1.9914e-03,  1.0584e-02, -4.0189e-03])\n",
      "tensor([[-0.4265,  0.0836,  0.0721,  ..., -0.4361,  0.0479, -0.4447],\n",
      "        [ 0.2039,  0.0745,  0.0634,  ...,  0.1870,  0.0827,  0.1878],\n",
      "        [ 0.1199,  0.0509,  0.0436,  ...,  0.1089,  0.0549,  0.1090],\n",
      "        ...,\n",
      "        [ 0.1144,  0.0496,  0.0428,  ...,  0.1040,  0.0532,  0.1039],\n",
      "        [ 0.1004,  0.0364,  0.0309,  ...,  0.0920,  0.0406,  0.0924],\n",
      "        [-0.2852, -0.1456, -0.0754,  ..., -0.2022, -0.1905, -0.2237]])\n",
      "tensor([-0.3434,  0.2769,  0.1696, -0.5005,  0.1642,  0.1753,  0.2130,  0.1628,\n",
      "         0.1362, -0.4541])\n",
      "tensor([[-5.9255e-03, -5.9252e-03, -5.9249e-03,  ..., -5.9165e-03,\n",
      "         -5.9162e-03, -5.9159e-03],\n",
      "        [ 2.0501e-03,  2.0442e-03,  2.0384e-03,  ...,  1.8805e-03,\n",
      "          1.8746e-03,  1.8688e-03],\n",
      "        [ 2.3427e-03,  2.3427e-03,  2.3427e-03,  ...,  2.3427e-03,\n",
      "          2.3427e-03,  2.3427e-03],\n",
      "        ...,\n",
      "        [ 7.0074e-04,  3.5227e-04,  3.8068e-06,  ..., -9.4048e-03,\n",
      "         -9.7533e-03, -1.0102e-02],\n",
      "        [ 2.5619e-03,  3.3242e-03,  4.0865e-03,  ...,  2.4669e-02,\n",
      "          2.5431e-02,  2.6193e-02],\n",
      "        [-3.4918e-03, -3.4963e-03, -3.5009e-03,  ..., -3.6227e-03,\n",
      "         -3.6272e-03, -3.6317e-03]])\n",
      "tensor([-2.1000e-03, -2.4149e-03, -4.7471e-03,  1.8195e-03, -3.0208e-03,\n",
      "        -1.5004e-03,  1.7470e-03,  4.4897e-03, -4.6868e-03, -4.3105e-03,\n",
      "         9.4013e-04, -8.7316e-04, -2.6598e-03,  1.8245e-03,  6.0240e-04,\n",
      "        -3.3299e-03, -7.6377e-03,  9.3953e-03,  7.7175e-03,  6.4383e-03,\n",
      "        -1.9766e-03,  4.1844e-03,  1.1237e-02, -7.3580e-03, -6.6197e-03,\n",
      "        -1.5482e-03, -6.2764e-03,  4.1583e-03, -6.3827e-04,  2.7127e-03,\n",
      "        -1.1002e-02,  1.6643e-03,  7.6189e-03,  6.0585e-03,  3.1560e-03,\n",
      "        -6.9747e-05, -6.4153e-03, -1.0988e-02,  5.0511e-03, -5.0119e-03,\n",
      "         7.9282e-03,  2.1348e-03,  2.8302e-03, -7.8369e-03, -3.1542e-03,\n",
      "        -9.1891e-05, -7.3795e-03, -2.1413e-03,  2.0942e-03,  5.8748e-04,\n",
      "         4.5367e-03, -6.0247e-03, -1.3525e-02, -7.6022e-03,  6.7013e-04,\n",
      "         6.1592e-03,  7.7162e-03, -3.4325e-04, -1.8184e-03,  1.5441e-03,\n",
      "        -4.0974e-03, -2.0951e-03,  3.8093e-05,  1.3242e-03,  1.6624e-03,\n",
      "        -3.6945e-03, -5.0166e-03,  6.0936e-04, -5.4959e-03,  2.6684e-03,\n",
      "        -1.1182e-03,  3.3520e-03,  4.9249e-03,  3.3509e-03,  2.4671e-03,\n",
      "        -3.8285e-03, -1.5653e-03,  1.7803e-03, -1.6577e-04,  6.6400e-03,\n",
      "        -8.3083e-04,  4.4862e-03, -3.3887e-03,  3.2356e-03, -9.7528e-03,\n",
      "         1.0786e-02,  2.2551e-03,  3.8904e-03,  3.9129e-04,  9.4325e-04,\n",
      "        -6.1745e-03, -5.5443e-03, -2.7359e-03,  1.0981e-03,  5.1030e-04,\n",
      "        -2.1313e-03,  7.6526e-03,  3.8139e-03, -9.5370e-03,  1.8283e-04,\n",
      "         9.5392e-03,  7.1301e-03,  3.1314e-03, -7.0155e-04,  7.8991e-04,\n",
      "        -8.9502e-04,  5.6708e-03,  3.9138e-03,  7.6671e-03, -5.4279e-03,\n",
      "        -2.4635e-03,  1.2210e-02,  7.9176e-03,  7.7904e-03, -1.1618e-02,\n",
      "         1.8521e-04,  4.2403e-03,  3.6953e-03,  1.6357e-03,  6.9411e-03,\n",
      "        -1.2803e-03,  1.0819e-02, -5.7625e-03,  1.1821e-02, -4.6331e-03,\n",
      "        -9.9568e-04,  5.2922e-03, -2.0095e-03])\n",
      "tensor([[-0.2133,  0.0418,  0.0360,  ..., -0.2181,  0.0240, -0.2224],\n",
      "        [ 0.1020,  0.0372,  0.0317,  ...,  0.0935,  0.0413,  0.0939],\n",
      "        [ 0.0599,  0.0254,  0.0218,  ...,  0.0544,  0.0275,  0.0545],\n",
      "        ...,\n",
      "        [ 0.0572,  0.0248,  0.0214,  ...,  0.0520,  0.0266,  0.0520],\n",
      "        [ 0.0502,  0.0182,  0.0155,  ...,  0.0460,  0.0203,  0.0462],\n",
      "        [-0.1426, -0.0728, -0.0377,  ..., -0.1011, -0.0952, -0.1119]])\n",
      "tensor([-0.1717,  0.1384,  0.0848, -0.2503,  0.0821,  0.0877,  0.1065,  0.0814,\n",
      "         0.0681, -0.2270])\n"
     ]
    }
   ],
   "source": [
    "# if I forward prop and backward prop again, gradients accumulate :\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)\n",
    "\n",
    "# you can remove this behavior by reinitializing the gradients in your network parameters :\n",
    "net.zero_grad()\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did backpropagation, but still didn't perform gradient descent. Let's define an optimizer on the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[ 0.1683,  0.1271, -0.0570,  ...,  0.0509,  0.0029,  0.1396],\n",
      "        [ 0.0897, -0.0470,  0.1538,  ...,  0.0113,  0.1658, -0.1457],\n",
      "        [-0.1631, -0.0063, -0.1529,  ..., -0.1409, -0.1373,  0.0114],\n",
      "        ...,\n",
      "        [ 0.0177, -0.0034,  0.0404,  ...,  0.1315, -0.1568, -0.0630],\n",
      "        [ 0.1281, -0.1212, -0.1050,  ...,  0.0757, -0.1225,  0.0942],\n",
      "        [-0.1489,  0.0763, -0.0951,  ...,  0.0506, -0.0818,  0.1764]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1180,  0.0759,  0.0698, -0.1238,  0.1715, -0.1681, -0.0615,  0.0139,\n",
      "         0.0282,  0.0104, -0.0972, -0.0423,  0.0740,  0.0780,  0.0622,  0.0414,\n",
      "         0.0018,  0.0042, -0.1281,  0.0354, -0.0453,  0.0674, -0.1139,  0.0806,\n",
      "         0.0038, -0.0815,  0.0847,  0.0365, -0.0064, -0.0384, -0.1690,  0.0996,\n",
      "         0.0952,  0.0341, -0.0457, -0.0309,  0.0196,  0.0274, -0.1702, -0.0458,\n",
      "         0.0352, -0.1366,  0.1607, -0.1137, -0.0695,  0.0403,  0.0866, -0.0381,\n",
      "        -0.0676, -0.1215,  0.1454,  0.1548,  0.0473,  0.0093, -0.0777,  0.1307,\n",
      "         0.0738,  0.0428, -0.0367,  0.1464,  0.1611,  0.0209,  0.0431, -0.1234,\n",
      "        -0.0167,  0.1560, -0.0413, -0.1622,  0.1103, -0.0161, -0.0406, -0.0504,\n",
      "         0.0779,  0.0155,  0.0946,  0.0614, -0.1548,  0.0500, -0.1348,  0.0805,\n",
      "        -0.0520,  0.0702, -0.1589,  0.0230,  0.1121, -0.0054, -0.0841, -0.0954,\n",
      "         0.1534,  0.1668,  0.0355, -0.1320,  0.0758,  0.0252, -0.1717, -0.1480,\n",
      "         0.1100, -0.0301, -0.1244,  0.0388,  0.1223,  0.1289, -0.0404,  0.0010,\n",
      "         0.1011, -0.0074,  0.1627,  0.1615, -0.1063, -0.0807,  0.0380, -0.0965,\n",
      "        -0.0350, -0.0039, -0.0983, -0.0435, -0.0097, -0.0312, -0.1722,  0.0109,\n",
      "         0.0997, -0.0460,  0.0446,  0.0779,  0.0752,  0.0099, -0.1635, -0.1513],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0660, -0.0163, -0.0678,  ...,  0.0781, -0.0618,  0.0480],\n",
      "        [ 0.0228, -0.0571,  0.0593,  ..., -0.0572, -0.0037,  0.0804],\n",
      "        [ 0.0182, -0.0647,  0.0140,  ...,  0.0525,  0.0714, -0.0430],\n",
      "        ...,\n",
      "        [-0.0343, -0.0520,  0.0450,  ..., -0.0847, -0.0734,  0.0746],\n",
      "        [ 0.0026, -0.0310, -0.0450,  ..., -0.0737, -0.0796, -0.0495],\n",
      "        [ 0.0613, -0.0553, -0.0587,  ..., -0.0024, -0.0569,  0.0634]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0863, -0.0429,  0.0670, -0.0347,  0.0024,  0.0866, -0.0604,  0.0735,\n",
      "        -0.0140,  0.0829], requires_grad=True)\n",
      "Parameters after gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[ 0.1684,  0.1271, -0.0570,  ...,  0.0510,  0.0030,  0.1396],\n",
      "        [ 0.0897, -0.0470,  0.1538,  ...,  0.0113,  0.1658, -0.1457],\n",
      "        [-0.1631, -0.0064, -0.1530,  ..., -0.1409, -0.1373,  0.0114],\n",
      "        ...,\n",
      "        [ 0.0177, -0.0034,  0.0404,  ...,  0.1316, -0.1567, -0.0629],\n",
      "        [ 0.1280, -0.1213, -0.1050,  ...,  0.0755, -0.1227,  0.0940],\n",
      "        [-0.1489,  0.0763, -0.0951,  ...,  0.0506, -0.0818,  0.1764]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1180,  0.0760,  0.0699, -0.1238,  0.1715, -0.1680, -0.0615,  0.0138,\n",
      "         0.0283,  0.0104, -0.0972, -0.0423,  0.0740,  0.0780,  0.0622,  0.0414,\n",
      "         0.0018,  0.0041, -0.1282,  0.0353, -0.0453,  0.0674, -0.1140,  0.0807,\n",
      "         0.0039, -0.0815,  0.0848,  0.0365, -0.0064, -0.0384, -0.1689,  0.0995,\n",
      "         0.0951,  0.0341, -0.0457, -0.0309,  0.0196,  0.0275, -0.1703, -0.0458,\n",
      "         0.0352, -0.1366,  0.1607, -0.1136, -0.0694,  0.0403,  0.0866, -0.0380,\n",
      "        -0.0676, -0.1215,  0.1454,  0.1549,  0.0474,  0.0094, -0.0777,  0.1306,\n",
      "         0.0737,  0.0428, -0.0367,  0.1463,  0.1611,  0.0210,  0.0431, -0.1234,\n",
      "        -0.0167,  0.1561, -0.0413, -0.1622,  0.1104, -0.0161, -0.0406, -0.0505,\n",
      "         0.0778,  0.0155,  0.0946,  0.0615, -0.1548,  0.0500, -0.1348,  0.0805,\n",
      "        -0.0520,  0.0702, -0.1589,  0.0229,  0.1122, -0.0055, -0.0842, -0.0954,\n",
      "         0.1534,  0.1668,  0.0356, -0.1319,  0.0758,  0.0252, -0.1717, -0.1479,\n",
      "         0.1099, -0.0301, -0.1243,  0.0388,  0.1222,  0.1288, -0.0405,  0.0010,\n",
      "         0.1011, -0.0074,  0.1627,  0.1615, -0.1064, -0.0806,  0.0381, -0.0966,\n",
      "        -0.0351, -0.0040, -0.0982, -0.0435, -0.0098, -0.0312, -0.1723,  0.0108,\n",
      "         0.0997, -0.0461,  0.0447,  0.0778,  0.0752,  0.0099, -0.1636, -0.1513],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0639, -0.0168, -0.0681,  ...,  0.0803, -0.0620,  0.0503],\n",
      "        [ 0.0217, -0.0574,  0.0590,  ..., -0.0581, -0.0041,  0.0794],\n",
      "        [ 0.0176, -0.0650,  0.0138,  ...,  0.0519,  0.0712, -0.0435],\n",
      "        ...,\n",
      "        [-0.0349, -0.0522,  0.0448,  ..., -0.0852, -0.0736,  0.0741],\n",
      "        [ 0.0021, -0.0312, -0.0452,  ..., -0.0741, -0.0798, -0.0500],\n",
      "        [ 0.0627, -0.0546, -0.0584,  ..., -0.0014, -0.0559,  0.0645]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0846, -0.0443,  0.0662, -0.0322,  0.0015,  0.0857, -0.0614,  0.0727,\n",
      "        -0.0147,  0.0852], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Parameters before gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Parameters after gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0752, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9747, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8850, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8045, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7319, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6661, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6066, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5526, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5034, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4583, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4168, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3782, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3424, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3088, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2773, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2477, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2197, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1934, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1685, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1450, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1228, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1017, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0818, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0628, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0447, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0275, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9954, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9804, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9523, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9264, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8912, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8697, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8596, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8497, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8402, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8310, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8051, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7970, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7891, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7814, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7740, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7596, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7528, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7461, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7332, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7209, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7036, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6980, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6874, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6822, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6771, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6722, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6673, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6626, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6489, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6402, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6359, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6318, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6277, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6237, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6198, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6084, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6047, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5975, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5940, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5872, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5838, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5805, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5773, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5679, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5559, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5530, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5474, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5446, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5418, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5365, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5338, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5312, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5287, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5261, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5236, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5211, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5069, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5046, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4978, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4957, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4935, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4913, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4871, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4850, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4830, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4789, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4769, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4730, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4691, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4634, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4615, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4543, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4525, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4507, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4490, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4455, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4438, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4421, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4404, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4355, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4338, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4322, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4306, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4290, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4259, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4243, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4227, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4212, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4197, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4182, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4093, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4064, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4050, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4036, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3994, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3980, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3953, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3939, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3912, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3899, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3886, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3872, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3859, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3846, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3833, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3821, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3808, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3795, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3783, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3770, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3745, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3733, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3721, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3708, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3636, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3625, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3613, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3601, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3590, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3578, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3567, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3555, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3544, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3522, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3510, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3488, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3477, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3466, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3455, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3434, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3412, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3402, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3349, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3329, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3288, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3218, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3199, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3180, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3161, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3095, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3086, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3077, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3067, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3058, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3040, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3031, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2996, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2978, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2969, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2961, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2952, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2943, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2935, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2918, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2909, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2901, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2884, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2876, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2867, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2859, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2851, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2834, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2826, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2818, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2794, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2786, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2770, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2762, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2754, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2746, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2739, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2723, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2715, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2708, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2700, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2692, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2685, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2654, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2632, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2625, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2617, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2610, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2603, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2595, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2581, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2574, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2567, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2552, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2538, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2531, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2510, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2503, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2490, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2483, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2469, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2462, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2456, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2442, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2435, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2416, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2409, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2402, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2370, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2351, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2344, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2332, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2325, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2319, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2313, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2307, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2294, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2288, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2276, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2199, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2175, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2136, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2086, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2081, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2075, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2070, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2065, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2054, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2044, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2028, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2007, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1997, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1992, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1982, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1977, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1972, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1967, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1962, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1952, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1947, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1942, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1932, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1927, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1917, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1908, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1898, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1893, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1884, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1875, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1856, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1851, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1847, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1842, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1833, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1829, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1824, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1815, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1811, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1806, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1797, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1793, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1789, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1784, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1780, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1763, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1758, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1754, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1746, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1737, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1733, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1716, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1712, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1708, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1700, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1692, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1664, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1644, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1636, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1629, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1625, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1621, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1617, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1610, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1606, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1602, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1599, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1595, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1591, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1587, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1580, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1576, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1573, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1566, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1562, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1558, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1555, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1551, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1548, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1544, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1540, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1537, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1530, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1523, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1520, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1516, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1513, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1509, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1506, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1496, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1492, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1489, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1486, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1482, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1479, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1469, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1466, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1462, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1459, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1456, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1453, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1446, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1443, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1440, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1433, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1430, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1427, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1424, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1421, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1418, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1415, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1411, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1408, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1405, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1402, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1369, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1366, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1351, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1349, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1346, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1343, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1340, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1337, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1334, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1331, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1329, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1317, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1315, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1312, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1309, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1306, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1298, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1295, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1293, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1290, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1287, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1284, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1279, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1276, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1268, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1261, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1255, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1253, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1250, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1245, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1243, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1237, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1235, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1232, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1230, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1227, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1225, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1220, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1217, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1213, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1208, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1205, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1203, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1200, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1198, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1195, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1188, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1186, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1181, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1179, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1172, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1165, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1160, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1149, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1131, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1127, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1090, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1088, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1086, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1084, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1082, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1080, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1076, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1074, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1072, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1070, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1068, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1066, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1064, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1060, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1058, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1056, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1054, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1052, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1050, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1048, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1046, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1044, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1042, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1040, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1036, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1034, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1033, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1031, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1029, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1027, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1021, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1019, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1018, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0999, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0997, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0996, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0994, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0992, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0990, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0987, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0983, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0981, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0980, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0976, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0974, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0967, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0962, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0957, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0955, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0954, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0952, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0947, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0945, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0944, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0941, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0939, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0937, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0934, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0932, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0929, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0928, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0924, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0923, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0921, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0918, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0915, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0913, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0909, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0907, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0904, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0902, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0899, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0898, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0896, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0895, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0893, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0890, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0886, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0881, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0878, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0875, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0874, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0873, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0871, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0868, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0867, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0865, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0864, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0861, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0860, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0854, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0853, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0848, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0846, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0842, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0839, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0833, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0829, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0827, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0821, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0816, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0811, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0801, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0800, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0797, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0793, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0792, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0783, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0782, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0774, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0764, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0762, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0761, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0757, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0753, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0752, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0731, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0726, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0722, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0719, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0715, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0701, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0687, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0685, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0681, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0657, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# In a training loop, we should perform many GD iterations.\n",
    "n_iter = 1000\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
    "    output = net(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.9902, -1.7800, -2.0774,  0.5787, -1.9603, -1.7135, -1.6187, -1.8039,\n",
      "         -1.8273,  4.1082],\n",
      "        [ 0.2567, -1.4705, -1.5120,  5.7903, -1.5307, -1.6327, -1.5679, -1.5251,\n",
      "         -1.6856,  3.2233],\n",
      "        [ 1.8633, -1.4923, -1.5976,  3.0309, -1.6929, -1.5278, -1.5327, -1.6354,\n",
      "         -1.6566,  5.6728]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "output = net(x)\n",
    "print(output)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to train a network ! For a complete training check the pytorch_example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n"
     ]
    }
   ],
   "source": [
    "# get dictionary of keys to weights using `state_dict`\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28,256),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(256,10))\n",
    "print(net.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save a dictionary\n",
    "torch.save(net.state_dict(),'test.t7')\n",
    "# load a dictionary\n",
    "net.load_state_dict(torch.load('test.t7'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common issues to look out for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4d1c8f2c4847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "net = nn.Linear(4,2)\n",
    "x = torch.tensor([1,2,3,4])\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.float()\n",
    "x = torch.tensor([1.,2.,3.,4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = 2* torch.ones(2,2)\n",
    "y = 3* torch.ones(2,2)\n",
    "print(x * y)\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected device cpu and dtype Float but got device cpu and dtype Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-d8d146283c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected device cpu and dtype Float but got device cpu and dtype Long"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,5)\n",
    "y = torch.arange(5)\n",
    "print(x+y)\n",
    "y = torch.arange(4).view(-1,1)\n",
    "print(x+y)\n",
    "y = torch.arange(4)\n",
    "print(x+y) # exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "print(x.t())\n",
    "print(x.view(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #4 'mat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7eb9531af49d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #4 'mat1'"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,120))\n",
    "x = torch.ones(256,2048)\n",
    "y = torch.zeros(256).long()\n",
    "net.cuda()\n",
    "x.cuda()\n",
    "crit=nn.CrossEntropyLoss()\n",
    "out = net(x)\n",
    "loss = crit(out,y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,n_hidden_layers):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.final_layer = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hidden.append(nn.Linear(128,128))\n",
    "    \n",
    "            \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.final_layer(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,n_hidden_layers):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.final_layer = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hidden.append(nn.Linear(128,128))\n",
    "        self.hidden = nn.ModuleList(self.hidden)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.final_layer(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL] *",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
