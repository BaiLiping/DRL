{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab 1: Markov Decision Processes - Problem 2\n",
    "\n",
    "\n",
    "## Lab Instructions\n",
    "All your answers should be written in this notebook.  You shouldn't need to write or modify any other files.\n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "\n",
    "*This project was developed by Peter Chen, Rocky Duan, Pieter Abbeel for the Berkeley Deep RL Bootcamp, August 2017. Bootcamp website with slides and lecture videos: https://sites.google.com/view/deep-rl-bootcamp/. It is adapted from Berkeley Deep RL Class [HW2](https://github.com/berkeleydeeprlcourse/homework/blob/c1027d83cd542e67ebed982d44666e0d22a00141/hw2/HW2.ipynb) [(license)](https://github.com/berkeleydeeprlcourse/homework/blob/master/LICENSE)*\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setup\n",
    "from misc import FrozenLakeEnv, make_grader\n",
    "env = FrozenLakeEnv()\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)\n",
    "class MDP(object):\n",
    "    def __init__(self, P, nS, nA, desc=None):\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "mdp = MDP( {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc)\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Policy Iteration\n",
    "\n",
    "The next task is to implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Below, you'll implement the first and second steps of the loop.\n",
    "\n",
    "### Problem 2a: state value function\n",
    "\n",
    "You'll write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You can solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vpi(pi, mdp, gamma):\n",
    "    # use pi[state] to access the action that's prescribed by this policy\n",
    "    V = np.ones(mdp.nS) # REPLACE THIS LINE WITH YOUR CODE\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the value of an arbitrarily-chosen policy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:  [ 1.381e-18  1.844e-04  1.941e-03  1.272e-03  2.108e-18  0.000e+00\n",
      "  8.319e-03  1.727e-16  3.944e-18  2.768e-01  8.562e-02 -7.242e-16\n",
      "  7.857e-18  3.535e-01  8.930e-01  0.000e+00]\n",
      "Actual:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "expected_val = np.array([  1.381e-18,   1.844e-04,   1.941e-03,   1.272e-03,   2.108e-18,\n",
    "         0.000e+00,   8.319e-03,   1.727e-16,   3.944e-18,   2.768e-01,\n",
    "         8.562e-02,  -7.242e-16,   7.857e-18,   3.535e-01,   8.930e-01,\n",
    "         0.000e+00])\n",
    "\n",
    "actual_val = compute_vpi(np.arange(16) % mdp.nA, mdp, gamma=GAMMA)\n",
    "if np.all(np.isclose(actual_val, expected_val, atol=1e-4)):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Expected: \", expected_val)\n",
    "    print(\"Actual: \", actual_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: state-action value function\n",
    "\n",
    "Next, you'll write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\sum_{s'} P(s,a,s')[ R(s,a,s') + \\gamma V^{\\pi}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "compute_qpi",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:  [[ 0.38   3.135  1.14   0.095]\n",
      " [ 0.57   3.99   2.09   0.95 ]\n",
      " [ 1.52   4.94   3.04   1.9  ]\n",
      " [ 2.47   5.795  3.23   2.755]\n",
      " [ 3.8    6.935  4.56   0.855]\n",
      " [ 4.75   4.75   4.75   4.75 ]\n",
      " [ 4.94   8.74   6.46   2.66 ]\n",
      " [ 6.65   6.65   6.65   6.65 ]\n",
      " [ 7.6   10.735  8.36   4.655]\n",
      " [ 7.79  11.59   9.31   5.51 ]\n",
      " [ 8.74  12.54  10.26   6.46 ]\n",
      " [10.45  10.45  10.45  10.45 ]\n",
      " [11.4   11.4   11.4   11.4  ]\n",
      " [11.21  12.35  12.73   9.31 ]\n",
      " [12.16  13.4   14.48  10.36 ]\n",
      " [14.25  14.25  14.25  14.25 ]]\n",
      "Actual:  [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def compute_qpi(vpi, mdp, gamma):\n",
    "    Qpi = np.zeros([mdp.nS, mdp.nA]) # REPLACE THIS LINE WITH YOUR CODE\n",
    "    return Qpi\n",
    "\n",
    "expected_Qpi = np.array([[  0.38 ,   3.135,   1.14 ,   0.095],\n",
    "       [  0.57 ,   3.99 ,   2.09 ,   0.95 ],\n",
    "       [  1.52 ,   4.94 ,   3.04 ,   1.9  ],\n",
    "       [  2.47 ,   5.795,   3.23 ,   2.755],\n",
    "       [  3.8  ,   6.935,   4.56 ,   0.855],\n",
    "       [  4.75 ,   4.75 ,   4.75 ,   4.75 ],\n",
    "       [  4.94 ,   8.74 ,   6.46 ,   2.66 ],\n",
    "       [  6.65 ,   6.65 ,   6.65 ,   6.65 ],\n",
    "       [  7.6  ,  10.735,   8.36 ,   4.655],\n",
    "       [  7.79 ,  11.59 ,   9.31 ,   5.51 ],\n",
    "       [  8.74 ,  12.54 ,  10.26 ,   6.46 ],\n",
    "       [ 10.45 ,  10.45 ,  10.45 ,  10.45 ],\n",
    "       [ 11.4  ,  11.4  ,  11.4  ,  11.4  ],\n",
    "       [ 11.21 ,  12.35 ,  12.73 ,   9.31 ],\n",
    "       [ 12.16 ,  13.4  ,  14.48 ,  10.36 ],\n",
    "       [ 14.25 ,  14.25 ,  14.25 ,  14.25 ]])\n",
    "\n",
    "Qpi = compute_qpi(np.arange(mdp.nS), mdp, gamma=0.95)\n",
    "if np.all(np.isclose(expected_Qpi, Qpi, atol=1e-4)):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Expected: \", expected_Qpi)\n",
    "    print(\"Actual: \", Qpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to run policy iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # chg actions | V[0]\n",
      "----------+---------------+---------\n",
      "\u001b[41m   0      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   0      |      1        | -0.00000\u001b[0m\n",
      "\u001b[41m   1      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   1      |      9        | 0.00000\u001b[0m\n",
      "\u001b[41m   2      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   2      |      2        | 0.39785\u001b[0m\n",
      "\u001b[41m   3      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   3      |      1        | 0.45546\u001b[0m\n",
      "\u001b[41m   4      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   4      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   5      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   5      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   6      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   6      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   7      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   7      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   8      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   8      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   9      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   9      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  10      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  10      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  11      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  11      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  12      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  12      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  13      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  13      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  14      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  14      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  15      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  15      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  16      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  16      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  17      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  17      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  18      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  18      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  19      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  19      |      0        | 0.53118\u001b[0m\n",
      "Test failed\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02c83cd438>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def policy_iteration(mdp, gamma, nIt, grade_print=print):\n",
    "    Vs = []\n",
    "    pis = []\n",
    "    pi_prev = np.zeros(mdp.nS,dtype='int')\n",
    "    pis.append(pi_prev)\n",
    "    grade_print(\"Iteration | # chg actions | V[0]\")\n",
    "    grade_print(\"----------+---------------+---------\")\n",
    "    for it in range(nIt):        \n",
    "        # YOUR CODE HERE\n",
    "        # you need to compute qpi which is the state-action values for current pi\n",
    "        vpi = compute_vpi(pi_prev, mdp, gamma)\n",
    "        qpi = compute_qpi(vpi, mdp, gamma)\n",
    "        pi = qpi.argmax(axis=1)\n",
    "        grade_print(\"%4i      | %6i        | %6.5f\"%(it, (pi != pi_prev).sum(), vpi[0]))\n",
    "        Vs.append(vpi)\n",
    "        pis.append(pi)\n",
    "        pi_prev = pi\n",
    "    return Vs, pis\n",
    "\n",
    "expected_output = \"\"\"Iteration | # chg actions | V[0]\n",
    "----------+---------------+---------\n",
    "   0      |      1        | -0.00000\n",
    "   1      |      9        | 0.00000\n",
    "   2      |      2        | 0.39785\n",
    "   3      |      1        | 0.45546\n",
    "   4      |      0        | 0.53118\n",
    "   5      |      0        | 0.53118\n",
    "   6      |      0        | 0.53118\n",
    "   7      |      0        | 0.53118\n",
    "   8      |      0        | 0.53118\n",
    "   9      |      0        | 0.53118\n",
    "  10      |      0        | 0.53118\n",
    "  11      |      0        | 0.53118\n",
    "  12      |      0        | 0.53118\n",
    "  13      |      0        | 0.53118\n",
    "  14      |      0        | 0.53118\n",
    "  15      |      0        | 0.53118\n",
    "  16      |      0        | 0.53118\n",
    "  17      |      0        | 0.53118\n",
    "  18      |      0        | 0.53118\n",
    "  19      |      0        | 0.53118\"\"\"\n",
    "\n",
    "Vs_PI, pis_PI = policy_iteration(mdp, gamma=0.95, nIt=20, grade_print=make_grader(expected_output))\n",
    "plt.plot(Vs_PI);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
